\ifx\allfiles\undefined
\documentclass[a4paper,12.15pt,CJK,oneside]{article}
\begin{document}
%\pagestyle{plain}
\else
\fi

\chapter{基于HORD算法优化MOM超参数的MOMWH模型}
\thispagestyle{fancy} \fancyhead[L]{\songti \wuhao
重庆师范大学硕士学位论文}\fancyhead[R]{\songti \wuhao 4~基于HORD算法对MOM的改进模型MOMWH}
{\songti\xiaosihao 本章主要介绍针对现有方法本文提出的问题问题三，本文设计的基于HORD算法对MOM的改进模型MOMWH，以下将从MOMWH模型的介绍，以及实验评估两个方面进行详细介绍。
\\
\section{基于HORD算法优化MOM超参数的MOMWH模型介绍}
{\songti\xiaosihao MOM模型中的子模型MOM-S是基于TransE构建的表示学习模型，MOM-R是基于TransR构建的表示学习模型，而无论是MOM-S，MOM-R模型，还是现有的TransE，TransR等表示学习模型得到的概念向量表示都非常依赖模型的参数。MOM中的MOM-S和MOM-R模型需调节的超参数包括训练轮数($epoch$)，学习率($r$，Learning rate)，向量维数($d$，Dimensions)，批次数($nbatch$)，以及负采样率($nrate$，Negative samping rate)，这些参数既包含整数也包含连续数；同时，模型本身的参数量较大，训练的时间成本也比较高，为进一步获得更好的参数，从而改进待匹配概念的表示并提高MOM本体匹配模型的效果，本节考虑采用HORD算法对MOM中的表示学习的超参数进行优化。

\indent HORD算法较现有的贝叶斯类超参数优化方法，如GP\cite{GP},TPE\cite{TPE}等方法，可以以更少的估值代价更加高效的同时优化整数或连续实数类超参数。而MOM中表示学习模型效果依赖于参数的选择，且参数既包含整数也包含连续实数，模型训练的时间成本也较高，而HORD算法较其他方法可以节省估值次数，同时优化整数以及连续实数超参，所以HORD算法是适合用于实现MOM模型超参数优化的方法。

%\centering
%\includegraphics[scale=0.6]{HORD.png}
%\caption{MOMWH模型框架}
%\label{fig:multiomhord}
%\end{figure}

\indent 为实现HORD算法对MOM中的表示学习模型参数的优化，进一步优化概念的向量表示，来提升MOM模型的性能，本节提出了如图\ref{fig:framework}的MOMWH模型框架。因基于上下文视角的预训练模型采用预训练好的词向量，并没有涉及参数优化，所以HORD算法只针对基于MOM-R（外部资源视角的预训练模型）以及基于MOM-S （结构视角的预训练模型）进行参数优化。MOMWH与MOM的不同之处在于MOM采用经验调节参数，来得到概念的向量表示，从而实现本体的匹配，而MOMWH采用HORD算法对MOM-R模型与MOM-S模型的超参数进行优化，将模型采用优化后的参数训练得到的向量作为概念的表示，进行后续的本体匹配。

\indent 采用HORD优化MOM中表示学习超参进而实现本体匹配的大致步骤如下：以MOM-R为例，第一步，设置HORD算法的参数，初始采样点的个数$n_0$，最大迭代步数$N_{max}$，以及确定模型各个参数可行域D；第二步，通过拉丁超立方体采样获得初始点$\{ {x_i}\} _{i = 1}^{{n_0}}$列，这里${x_i}$ 表示一组向量，即一组参数；第三步，采用每一组参数${x_i}$，以MOM中的数据集训练模型MOM-R，得到点对${{\rm A}_{{n_0}}} = \{ \{ {x_i},f({x_i})\} _{i = 1}^{{n_0}}$，$f({x_i})$表示模型训练后得到的验证误差值（实际可以是损失，错误率等）；第四步，采用${{\rm A}_{{n_0}}}$ 更新径向基响应面模型；第五步，通过当前响应面模型求得最优点${x_ * }$，再采用参数${x_ * }$训练MOM-R模型得到$f({x_ * })$，更新${{\rm A}_{{n_0}}} = {{\rm A}_{{n_0}}} \cup \{ ({x_ * },f({x_ * })\}$；第六步在未达到最大步数$N_{max}$时，重复步骤四至步骤五；最后得到改进后的参数以及概念的向量表示，再采用MOM中的多视角匹配算法得到最终的匹配。具体算法过程如算法\ref{alg:Framwork}所示。

\begin{algorithm}[!htb]
\caption{使用HORD算法优化MOM中表示学习模型超参的算法框架}
\label{alg:Framwork}
\begin{algorithmic}[1] %这个1 表示每一行都显示数字
\REQUIRE ~~\\ %算法的输入参数：Input
初始采样数${n_0} = 2(D + 1)$;\\
候选点个数$m = 100D$;\\
最大迭代步${N_{\max }}$;\\
初始迭代步$n = {n_0}$;
\ENSURE ~~\\ %算法的输出：Output
优化后的模型参数配置${x_{better}}$;\\
${x_{better}}$参数配置下的概念表示;
\STATE 使用拉丁超立方体采样$n_0$个初始超参配置，记$\{ {x_i}\} _{i = 1}^{{n_0}}$;
\STATE 采用$x_i$训练模型，得到验证误差$f({x_i})$，$i = 1, \ldots ,{n_0}$，记${{\rm A}_{{n_0}}} = \{ \{ {x_i},f({x_i})\}$;
\WHILE{$n < {N_{\max }}$}
    \STATE 使用${{\rm{A}}_n}$构建径向基响应面模型${S_n}(x)$;
    \STATE 计算当前最优点${x_{better}} = argmin\left\{ {f({x_i}):i = 1,...,n} \right\}$;
    \STATE 计算坐标扰动的概率${\varphi _n}$;
    \label{xuyao1}
    \STATE 构建由$m$个候选点${t_n}_{1:m}$组成的候选点集;\\
        对${t_n}_{1:m}$中每个候选点$y_i$:\\
        令${y_i}={x_{better}}$;\\
        按扰动概率${\varphi _n}$扰动$y_i$对应分量；\\
        对被扰动分量添加扰动${\delta _i}$ (${\delta _i} \sim N(0,{\sigma ^2})$);
    \label{xuyao2}
    \STATE 根据候选点计算$V_n^{ev}({t_n}_{1:m})$，$V_n^{dn}({t_n}_{1:m})$，${W_n}({t_n}_{1:m})$;
    \label{xuyao3}
    \STATE 根据当前${W_n}({t_n}_{1:m})$，计算当前最优点${x_ * } = argmin\left\{ {{W_n}({t_n}_{1:m})} \right\}$;
    \STATE 根据当前最优参数配置$x_ * $，训练模型，得到当前验证误差$f({x_ * })$;
    \STATE 调整方差${\delta _i}$;
    \STATE 更新点值对${{\rm A}_{{n_0}}} = {{\rm A}_{{n_0}}} \cup \{ ({x_ * },f({x_ * })\}$；
\ENDWHILE；
\label{code:fram:select}
\RETURN $x_*$，$x_*$参数配置下，概念向量表示; %算法的返回值
\end{algorithmic}
\end{algorithm}

\indent 其中，扰动概率${\varphi _n}$（对应步骤6）是关于迭代次数${n_0} \le n \le {N_{\max }} - 1$的严格递减函数，其计算表达式为：
\[{\varphi _n} = {\varphi _0}[1 - \frac{{\ln (n - {n_0} - 1)}}{{\ln ({N_{\max }} - {n_0})}}]{n_0} \le n \le {N_{\max }}\]
\indent 在步骤7和步骤8旨在从候选点中选出最可能的极小点作为新的采样点，通过计算每个候选点的${V^{ev}}(t)$和${V^{dm}}(t)$作为其度量标准，函数定义如下：
\[{V^{ev}}(t) = \left\{ {\begin{array}{*{20}{c}}
{\frac{{S(t) - {S^{\min }}}}{{{S^{\max }} - {S^{\min }}}}}&{{S^{\max }} \ne {S^{\min }}}\\
{\rm{1}}&{otherwise}
\end{array}} \right.\]
\[{V^{dm}}(t) = \left\{ {\begin{array}{*{20}{c}}
{\frac{{{\Delta ^{\max }} - \Delta (t)}}{{{\Delta ^{\max }} - {\Delta ^{\min }}}}}&{{\Delta ^{\max }} \ne {\Delta ^{\min }}}\\
{\rm{1}}&{otherwise}
\end{array}} \right.\]
其中$\Delta (t) = \mathop {\min }\limits_{1 \le i \le n} \left\| {t - {t_i}} \right\|$，候选点最终的得分为${V^{dm}}(t)$和${V^{ev}}(t)$的加权和，具体如公式\ref{equ:spec}所示,最后取得分最低的候选点为新的采样点。
\begin{equation}
\label{equ:spec}
W(t) = \omega {V^{ev}}(t) + (1 - \varpi ){V^{dm}}(t)
\end{equation}
}\\
\section{实验与评估}
\\
\subsection{数据集}
{\songti\xiaosihao 本节简要概述在本体匹配实验中使用的四种本体，其中两个本体FMA(Foundational Model of Anatomy)和MA(Adult Mouse
anatomical)分别表示解剖学本体和成年小鼠解剖学本体，二者都是纯粹的解剖学本体论，而另外两个SNOMED CT和NCI是涉及更广的生物医学本体，解剖学只是它们描述的一个子领域。虽然这些本体的最新版本是可用的，但是本文在整个本体匹配任务中参考了出现在OAEI(Ontology Alignment Evaluation Initiative) 中的版本，以便在本体匹配系统之间进行比较。

\indent FMA(Foundational Model of Anatomy)：这是一个一直被更新的本体，自1994年由华盛顿大学开发并维护\cite{FMA}，其目的是以机器可读的形式概念化人体的表型结构。

\indent MA(Adult Mouse Anatomical Dictionary)：该本体是描述成年小鼠解剖结构的结构的词汇表\cite{MA}。

\indent NCI(NCI Thesaurus)：该本体提供了癌症的标准词汇\cite{NCI}，其解剖学子域描述人类自有的生物结构、液体和物质。

\indent SNOMED(SNOMED Clinical Terms)：该本体是一个系统化，组织化的机器可读的医学术语集合，提供临床文档和报告中使用的代码、术语、同义词和定义\cite{SNOM}。

\indent 以上本体的概念及对应匹配数量具体如图\ref{tab:dataset}：
\begin{table}[h]
\centering
\caption{本体概念数及对应匹配数量}
\label{tab:dataset}
\renewcommand\arraystretch{1.5}
\begin{tabular}{ccccc}
\hline
 源本体&概念数&目标本体&概念数&匹配数\\
\hline
    MA&2744&NCI&3304&1489\\
\hline
 FMA&3696&NCI&6488&2504\\
\hline
  FMA&10157&SNOMED&13412&7774\\
\hline
\end{tabular}
\end{table}
}
\\
\subsection{评估指标}
  {\songti\xiaosihao
  OAEI为本体匹配提供了统一的评估标准，评估的指标主要是：匹配的准确率，召回率和F1值。为阐述以上三个指标含义，首先介绍以下概念：

  \indent      TP(True Positives) ：表示模型将正例预测为正例的数据条数；

  \indent      FP(False Positives)：表示模型将负例预测为正例的数据条数；

  \indent      FN(False Negatives)：表示模型将正例预测为负例的数据条数；

  \indent      TN(True Negatives) ：表示模型将负例预测为负例的数据条数；

\indent 所以，由以上四个参数值，可根据以下公式求得具体指标的值，其中total表示待预测数据的总数。

  \indent     准确率(Acc)：$Acc = \frac{{TP + TN}}{{{\rm{total}}}}$

  \indent     召回率(Rec)：$Rec = \frac{{TP}}{{TP + FN}}$

  \indent     F1       值：$F1 = \frac{{2 * Acc * Rec}}{{Acc + Rec}}$}

\\
\subsection{实验环境}
{\songti\xiaosihao 为了验证本文工作的有效性，本文使用Python借助TensorFlow\footnote{https://www.tensorflow.org/}深度学习框架来实现本文提出的方法。采用OWLAPI\footnote{http://owlapi.sourceforge.net/}（用于管理OWL本体的工具）本体解析工具来获得本体信息。实验是在具有64GB内存和TiTAN XP GPU 的Intel Xeon E5-2630 V4 CPU的个人工作站上进行的。}
\\
\subsection{对比方法}

\begin{table}[h]
\centering
\caption{实验对比方法表}
\label{tab:dataset}
\renewcommand\arraystretch{1.5}
\begin{tabular}{ll}
\hline
 模型名称&模型含义\\
\hline
MOM-S&MOM模型中基于结构负采样视角的子模型效果\\
MOM-L&MOM模型中基于上下文视角的子模型效果\\
MOM-R&MOM模型中基于外部资源视角的子模型效果\\
MOM-SR&MOM基于外部资源模型的效果与结构负采样模型效果的融合结果\\
MOMWH-S&MOMWH模型中基于结构负采样视角的子模型效果\\
MOMWH-L&MOMWH模型中基于上下文视角的子模型效果\\
MOMWH-R&MOMWH模型中基于外部资源视角的子模型效果\\
MOMWH-SR&MOMWH基于外部资源模型的效果与结构负采样模型效果的融合结果\\
MOM&本文提出的本体匹配模型（Multi-view Ontology Matching Model）效果\\
MOM$^-$&MOM模型未采用结构负采样的结果\\
MOM$^+$&在MOM基础上采用BERT模型改进其预训练词向量表示的模型效果\\
MOMWH$^-$&MOMWH模型未采用结构负采样的结果\\
MOMWH&采用HORD算法对MOMM超参进行优化的模型效果\\
MOMWH$^+$&在MOM基础上采用BERT模型改进其预训练词向量表示的模型效果\\
\hline
\end{tabular}
\end{table}
\\
\subsection{实验结果}

{\songti\xiaosihao 针对现有方法的本文提出的第三个问题，本文设计如下实验，逐步验证MOMWH得建模思想是否正确，是否可以有效解决现有方法的这个问题，具体实验步骤由两部分构成，分别是MOMWH与MOM的效果对比实验和MOMWH$^+$与MOMWH，MOM及当前最好方法的效果对比实验。

\indent 本体概念中单词向量表示主要来自文献\cite{BIO}的链接\footnote{https://doi.org/10.5281/zenodo.1173936}，其维数设置为200。对于一些没有向量表示的单词，我们将其随机初始化，并满足${\left|{\left|{{t_{1i}}}\right|}\right|_2}{\rm{ }} \le {\rm{ }}1$ 和${\left|{\left|{{t_{2i}}}\right|}\right|_2}{\rm{ }} \le {\rm{ }}1$。}
\\
\noindent \textbf{1.MOMWH与MOM的效果对比实验}

{\songti\xiaosihao 本实验针对MA-NCI-FMA和FMA-NCI-small-SNOMED两个数据集，将本文的MOMWH，MOMWH$^-$，MOM，MOM$^-$以及MOMWH和MOM中多个不同视角的模型，将MOMWH与MOM中相对应的模型进行对比，以上方法中唯一的变量因素为MOM中的模型参数是人工调节的参数，MOMWH中模型参数一部分为采用HORD算法优化得到的参数，通过该实验来验证HORD算法是否可以改进表示学习模型学得的概念向量表示，是否可以因此提升本体匹配的效果。其中，MOM方法训练及其匹配算法的参数如图\ref{tab:canhsu}，MOMWH模型训练时的参数如图\ref{tab:canhsu2}，实验中也对比了对于同一视角的模型在不同的阈值下模型的匹配效果。

\indent 该实验首先需采用HORD算法对MOM中的表示学习模型MOM-S，MOM-R进行超参数优化。HORD算法的执行与MOM-L或MOM-R训练的方式有所不同，需给出模型（MOM-S，MOM-R类模型）中待优化超参数的区间范围，而不用给定具体的值。对于模型中没有进行进一步优化的参数，则采用MOM中对应模型相同的参数。
\begin{table}[!h]
\label{tab:canhsu2}
\caption{MOMWH模型中HORD超参调节的参数设置表}
\centering
\renewcommand\arraystretch{1.5}
\begin{tabular}{cc}
\toprule  %添加表格头部粗线
\midrule  %添加表格中横线
参数     & 参数值 \\
\hline
$d$      & $\{{\rm{50}},{\rm{100}}\}$ \\
$d_M$    & $\{ {\rm{50}} \times {\rm{50}},{\rm{100}} \times {\rm{100}}\}$ \\
$nbatch$ & $\left[ {10,60} \right]$\\
$r$      & $\left[ {0.001,0.05} \right]$\\
$nrate$  & $\left[ {3,15} \right]$\\
$epoch$  & $500$\\
$n_0$    & $6$ \\
$N_{max}$& $25$\\
\bottomrule %添加表格底部粗线
\end{tabular}
\end{table}

\indent MOM-S和MOM-R的超参数包括训练轮数($epoch$)，学习率($r$)，向量维数($d$)，批次数($nbatchs$)，以及负采样率($nrate$)。为降低参数调节的计算量，提升MOMWH模型效率，本文未对以上六种参数都进行优化。首先，MOM中的表示学习模型的参数规模由向量维数大致确定，而参数的规模应与本体数据集中的概念以及关系的多少有关，在二者数量不变的前提下，可大致确定向量的维数($d$)，保证可以适用于匹配任务即可。所以，MOM中未对这两个参数进行调节；其次，训练轮数($epoch$)根据人工经验得到的值为1000，该参数决定了模型学习时间的长短，为保证参数优化后，可以提升MOM的的效率，我们将其固定为500，通过调节剩余的三个参数：学习率（$r$），批次数（$nbatchs$）和负采样率($nrate$)。所以MOMWH模型的参数设置如表\ref{tab:canhsu2} 所示。

\indent MOMWH中通过HORD算法进行超参优化后，MOM-S与MOM-R的最优超参配置对比如表\ref{tab:canhsu3}所示：
\begin{table}[!h]
\label{tab:canhsu3}
\caption{HORD算法进行超参数优化前后的参数对比表}
\centering
\renewcommand\arraystretch{1.5}
\begin{tabular}{ccc|cc}
\toprule  %添加表格头部粗线
\midrule  %添加表格中横线
参数名   & \multicolumn{2}{c|}{MOM-S}&\multicolumn{2}{c}{ MOM-R}\\
         & 优化前   & 优化后   & 优化前  & 优化后 \\
\hline
$nbatch$ &   5      &   8      &  10     &   12 \\
$r$      &   0.0100 &   0.0131 &  0.0100 &   0.0107\\
$nrate$  &   3      &   6      &  5      &   8\\
$epoch$  &   1000   &   500    &  1000   &   500\\
\bottomrule %添加表格底部粗线
\end{tabular}
\end{table}

\indent 除了以上经过HORD算法优化过的参数外，其他参数依然与原模型中参数相同。MOMWH与MOM的效果对比如表\ref{tab:result5}，\ref{tab:result10}，\ref{tab:result6}所示。
\begin{table}[!h]
	\centering
	\footnotesize
	\caption{MOMWH与MOM在MA-NCI下的效果对比表}
	\label{tab:result5}
    \renewcommand\arraystretch{1.5}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Methods&\multicolumn{5}{c|}{MA-NCI}\\
		&Number & Correct& P &R & F1\\\hline
		MOM$^-$&1484 & 1296   & 0.873  & 0.855 &  0.864\\\hline
		MOM&1445 & 1287   & 0.891  & 0.849 &  0.869\\\hline
        MOMWH$^-$&1462 & 1296   & 0.886  & 0.855 &  0.870\\\hline
        MOMWH&1431 & 1290   & 0.901  & 0.851 &  0.0.876\\\hline
	\end{tabular}
\end{table}

\begin{table}[!h]
	\centering
	\footnotesize
	\caption{MOMWH与MOM在FMA-NCI-small下的效果对比表}
	\label{tab:result10}
    \renewcommand\arraystretch{1.5}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Methods& \multicolumn{5}{c|}{FMA-NCI-small} \\
		&Number & Correct& P &R & F1\\\hline
		MOM$^-$&  2471   &  2192  &  0.887  & 0.809   & 0.846    \\\hline
		MOM&  2470  & 2195  &    0.889 &   0.817 &   0.0.851  \\\hline
        MOMWH$^-$&  2469   &  2192  &  0.888  & 0.809   & 0.847    \\\hline
        MOMWH&  2466  & 2200  &    0.892 &   0.819 &   0.854  \\\hline
	\end{tabular}
\end{table}

\indent 如表\ref{tab:canhsu3}所示，采用HORD算法进行超参优化后与优化前的模型参数对比，对比可以发现，超参数优化后，对于同样的模型之前训练需要1000个$epoch$，而优化后只需要500个$epoch$，较之前只需要一半的训练次数，极大的提升了模型的效率。

\indent 对比表\ref{tab:result5}，\ref{tab:result10}中的数据，MOMWH无论是准确率，召回率还是F值都较之前的MOM都有所提升，MOMWH$^-$的性能也优于MOM$^-$。MOMWH是基于MOM采用HORD算法进行超参优化的模型，显然，经过优化后的参数，可以学到更加准确的特征；另外，无论是MOM与MOM$^-$还是MOMWH与MOMWH$^-$ 都说明同利用了改进后的结构负采样，效果依然会有所提高，也进一步验证了本体的结构信息有助于本体匹配任务效果的提升。

\begin{table}[!h]
	\centering
	\caption{MOMWH与MOM不同视角模型及其组合后在MA-NCI下的效果对比表}
	\footnotesize
	\label{tab:result6}
    \renewcommand\arraystretch{1.5}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Methods &Number & Correct& P &R & F1\\\hline
		MOM-$S^-$ (threshold$=0.95$)  & 1836& 1109      &    0.604   & 0.732  & 0.662    \\\hline
		MOM-$S$ (threshold$=0.95$)    & 1189& 1097& 0.923  & 0.724 &   0.811  \\\hline
		MOM-$R$ (Random initialization, threshold$=0.95$) &    709   &   680       &  0.959   &  0.449   &  0.612      \\\hline
		MOM-$R$ (loss function,  threshold$=0.95$)     & 833& 789   & 0.948    & 0.520 &   0.672  \\\hline
		MOM-$RS^-$ (threshold$=0.95$)   & 1271& 1147& 0.902  & 0.757 &   0.823  \\\hline
		MOM-$RS$ (threshold$=0.95$)   & 1237& 1138& 0.920  & 0.751 &   0.827  \\\hline
        MOM$^-$&1484 & 1296   & 0.873  & 0.855 &  0.864 \\\hline
        MOM&1445 & 1287   & 0.891  & 0.849 &  0.869  \\\hline
        MOMWH-$S^-$ (threshold$=0.95$)  & 1821& 1110      &    0.610   & 0.733  & 0.666    \\\hline
		MOMWH-$S$ (threshold$=0.95$)    & 1170& 1099& 0.939  & 0.725 &   0.818  \\\hline
		MOMWH-$R$ (Random initialization, threshold$=0.95$) &    705   &   680       &  0.965   &  0.449   &  0.613      \\\hline
		MOMWH-$R$ (loss function,  threshold$=0.95$)     & 824& 794   & 0.964    & 0.524 &   0.679  \\\hline
		MOMWH-$RS^-$ (threshold$=0.95$)   & 1263& 1147& 0.908  & 0.757 &   0.826  \\\hline
		MOMWH-$RS$ (threshold$=0.95$)   & 1221& 1140& 0.934  & 0.752 &   0.833  \\\hline
		MOMWH$^-$&1462 & 1296   & 0.886  & 0.855 &  0.870 \\\hline
		MOMWH&1431 & 1290   & 0.901  & 0.851 &  0.876  \\\hline
	\end{tabular}
\end{table}}


\indent 表\ref{tab:result6}展示了MOMWH与MOM分别将不同视角的模型组合后的效果对比。通过各个视角模型的匹配结果详细对比可以发现，采用HORD算法进行参数优化后，基本上对各个视角的匹配结果都有所提升。众多的结果说明，经过参数优化后，不仅可以使得各个模型错误的匹配得以减少同时可以寻找到一些正确匹配，以上结果进一步说明参数优化后可以帮助表示学习模型学得更加合适得特征，获得得概念向量表示更加有效。
\\
\noindent \textbf{2.MOMWH$^+$与MOMWH，MOM及当前最好方法的效果对比实验}

{\songti\xiaosihao 本实验依然针对实验一中的两个数据集，将本文的MOMWH$^+$等方法与当前最好的方法:SCBOW+DAE\cite{scbow}，LogMapBio\cite{logmapbio}，POMAP++\cite{logmapbio}，XMap\cite{xmap}，LogMap\cite{lm}\\FCAMapX\cite{fca}，SANO-M\cite{logmapbio}，
的匹配效果进行比较。根据之前的实验，发现利用改进后的TF-IDF算法的效果准确率有$0.991$, 其召回率不高，主要是因为我们利用的是预训练词向量，只包含其字面语义信息，并未对这些预训练词向量基于当前语境进行改进，所以而未包含该本体匹配任务下的语义信息。考虑到以上原因，本文基于MOM框架，提出采用BERT模型对其预训练词向量进行进一步改进的MOM$^+$，并采用HORD算法进行超参数优化的MOMWH$^+$模型，具体的对比结果如表\ref{tab:result8} 所示。

\indent 通过比较表\ref{tab:result8}，\ref{tab:result11}中的数据可以发现，MOMWH$^+$的效果较MOMWH和MOM有了较大幅度的提升，并且从召回率与F值来综合比较，MOMWH$^+$的效果比LogMapBio，POMAP++等方法都要有优势。另外与第一名的SCBOW+DAE相比，在FMA-NCI-small-SNOMED数据集中的效果，也与之可以相提并论。

\begin{table}[!h]
	\centering
	\caption{本文模型与当前最好方法在MA-NCI下的效果对比表}
	\label{tab:result8}
	\footnotesize
    \renewcommand\arraystretch{1.5}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Methods &\multicolumn{5}{c|}{MA-NCI}\\
		  &Number & Correct& P &R & F1 \\\hline
		SCBOW + DAE 	& 1399 & 1356   & 0.969  & 0.906 &   0.938\\\hline
        \textbf{MOMWH$^+$}& \textbf{1436} & \textbf{1363}  & \textbf{0.949}  & \textbf{0.891} &  \textbf{0.919}\\\hline
		\textbf{MOM$^+$}& \textbf{1441} & \textbf{1363}  & \textbf{0.946}  & \textbf{0.891} &  \textbf{0.918}\\\hline
        LogMapBio   &1550 & 1376  &  0.888& 0.908 & 0.898\\\hline
		POMAP++     &1446 &  1329 &  0.919& 0.877 & 0.897\\\hline
		XMap             &1413 &   1312& 0.929 & 0.865&  0.896\\\hline
		LogMap        & 1387&   1273  & 0.918 & 0.846&   0.880\\\hline
		SANOM         &1450 & 1287   & 0.888 & 0.844&  0.865\\\hline
		FCAMapX      & 1274&  1199   & 0.941 & 0.791 &  0.859\\\hline
		\textbf{MOM}& \textbf{1445} & \textbf{1287}  & \textbf{0.891}  & \textbf{0.849} &  \textbf{0.869}\\\hline
	\end{tabular}
\end{table}

\begin{table}[!h]
	\centering
	\caption{本文模型与当前最好方法在FMA-NCI-small下的对比表}
	\label{tab:result11}
	\footnotesize
    \renewcommand\arraystretch{1.5}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Methods & \multicolumn{5}{c|}{FMA-NCI-small} \\
		  &Number & Correct& P &R & F1\\\hline
		SCBOW + DAE 	& 2282 & 2227  & 0.976 & 0.889    & 0.930 \\\hline
        \textbf{MOMWH$^+$}&  \textbf{2787}  & \textbf{2647}  &    \textbf{0.950} &   \textbf{0.907} &   \textbf{0.928} \\\hline
		\textbf{MOM$^+$}&  \textbf{2793}  & \textbf{2647}  &    \textbf{0.948} &   \textbf{0.907} &   \textbf{0.927} \\\hline
        LogMapBio  & 2776  & 2632  & 0.948  & 0.902   & 0.921 \\\hline
		POMAP++    &2414  &2363  & 0.979  & 0.814    & 0.889 \\\hline
		XMap       &2315  & 2262 & 0.977  & 0.783   & 0.869 \\\hline
		LogMap     & 2747 & 2593  & 0.944 &  0.897  & 0.920\\\hline
		SANOM      & --  & --  & -- & --   & -- \\\hline
		FCAMapX    & 2828 &  2681  & 0.948 & 0.911   & 0.929 \\\hline
		\textbf{MOM}&  \textbf{2330}  & \textbf{2195}  &    \textbf{0.942} &   \textbf{0.817} &   \textbf{0.875} \\\hline
	\end{tabular}
\end{table}

\indent 综上实验，可以进一步验证说明本文提出的方法，可以在一定程度上解决现有基于深度学习的方法的问题，本文充分利用本体中的结构信息，并且对本文提出的基于表示学习的方法MOM等模型进行超参数优化，以四个不同的实验，进一步验证了充分利用本体的结构信息有助于提升本体匹配的效果，采用多视角的建模方式是实现本体匹配任务的有效方式之一，以及采用HORD算法对表示学习模型的超参数进行优化，可以改进其获得概念向量表示，从而提升本体匹配的效果，本文模型MOMWH$^+$，MOMWH也取得了比较好的效果。
}

\ifx\allfiles\undefined
\end{document}
\fi
