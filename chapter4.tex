\ifx\allfiles\undefined
\documentclass[a4paper,12.15pt,CJK,oneside]{article}
\begin{document}
%\pagestyle{plain}
\else
\fi

\chapter{研究内容}
\thispagestyle{fancy} \fancyhead[L]{\songti \wuhao
重庆师范大学硕士学位论文}\fancyhead[R]{\songti \wuhao 4~研究内容}
\section{基于多视角的本体匹配模型MuitiOM}
\subsection{问题定义}
{\songti\xiaosihao 本体匹配是解决领域本体异构性问题的有效方法之一。为更好的阐述本文的研究内容，本文首先引入了本体匹配的形式化定义。

\indent \textbf{定义1：}本文采用四元组$({e_i},{e_j},r,\theta )$来表示$0_i$，$0_j$的一组匹配，其中$0_i$和$0_j$表示两个本体，$e_i$，$e_j$分别代表$0_i$ 和$0_j$ 中的实体（实体可以是本体的概念，性质等），r表示两个实体$e_i$，$e_j$之间的关系，在多数本体中，r一般表示$\left\{ { \subseteq , \supseteq , \equiv } \right\}$ 三类关系中的一种。

\indent 在很多本体匹配的实际任务中，匹配系统主要关注具有等价关系（等价关系可以理解为一般的同义词关系）的概念匹配。因此，在本文的剩余部分中，只针对需要获取等价关系的本体匹配任务，本文的模型设计也只注重挖掘领域本体中的存在等价关系的概念对。
\subsection{MuitiOM}
  {\songti\xiaosihao 现有的工作\cite{xian1,xian2,xian3}表明，采用多视角的方式可以从不同角度获得本体更多的语义信息，并提升相应任务的准确性和稳定性。受以上工作启发，本文采用基于多视角的方法来描述本体匹配的过程，并针对现有方法未充分利用本体结构信息的问题，尝试融入更多的结构信息，来提升匹配效果。

\begin{figure}[!h]
\centering
\includegraphics[scale=0.6]{multiom.png}
\caption{“MultiOM”模型框架}
\label{fig:muitiom}
\end{figure}

\indent MultiOM模型的框架如图\ref{fig:muitiom}所示，给定两个本体$0_1$和$0_2$，首先抽取本体中的概念，结构关系信息，随后将本体匹配任务分成基于外部资源视角，基于结构视角以及基于上下文视角的三个模块来实现。为构建模型数据集，采用字典来实现本体概念的规范化以及同义概念的挖掘；为获得本体所有概念所涵盖单词的语义信息，在上下文视角的模块引入了上下文信息，第三方本体的结构信息有助于提升匹配质量\cite{}，因此在外部资源视角模块中将第三方本体作为外部资源。基于三个视角的预训练模型得到概念的多组向量表示，最后通过相互评价的多视角匹配算法来得到最终的匹配。

\indent MultiOM与特征工程类的方法不同，是利用本体的概念的语义信息，通过表示学习技术得到概念的连续向量表示，避开了人工构建特征的麻烦。与现有的基于深度学习的方法相比，MultiOM利用第三方本体，以及本体自身的结构信息，来进一步提升匹配的效果。在MultiOM中，对同一个概念存在不同粒度的向量表示，在外部资源视角和结构视角，MultiOM分别基于TransE，TransR构建表示学习模型得到以每个概念整体为单位的向量表示，然后通过余弦相似度得到概念之间的相似度；上下文视角中，基于word2vec\cite{}训练上下文得到的本体概念中所有单词的向量表示，一般概念由多个单词的组成，将这些单词对应向量所构成的集合$\bf{\left\{{{t_1},{t_2}, \ldots ,{t_n}} \right\}}$来表示该概念，再利用这些单词表示，基于本文设计的算法计算概念之间的相似度。

\noindent {\textbf{1.基于上下文视角的预训练模型}}

\indent 基于上下文视角的预训练模型主要基于TF-IDF算法\cite{tfidf}，TF-IDF算法也是本体匹配中计算字符串相似度的非常有效的方法之一\cite{}。根据TF-IDF算法的假设，可将一个本体中所有概念涵盖的单词表示成一个由单词构成的词袋，每个概念${C_i}$看作一个文档。单个概念所包含的单词$\left\{ {{t_1},{t_2}, \ldots ,{t_n}} \right\}$看作术语。受软TF-IDF算法\cite{}的启发，本文提出了一种基于词向量嵌入的TF-IDF策略的改进算法来计算概念的相似性，再通过匹配算法实现本体匹配的模型。首先通过word2vec模型\cite{}训练对应上下文，得到词袋中所有单词的向量表示；然后利用概念中的单词的表示，得到概念的表示$\bf{\left\{ {{t_1},{t_2}, \ldots ,{t_l}} \right\}}$，这里$l$等于概念所包含的单词数量，最后基于概念表示（概念中的单词的表示构成的集合）利用本文设计的相似度计算方式得到概念对的相似度。这里的相似度计算方法不同于一般的字符串等价，相应的定义如公式\label{equ:1}：
\begin{equation}
Sim({C_1},{C_2}) = \sum\limits_{i = 1} {{w_i} \cdot \mathop {\arg \max }\limits_j \cos ({{\bf{t}}_{{\bf{1i}}}},{{\bf{t}}_{{\bf{2j}}}})}
\label{equ:1}
\end{equation}
其中，$C_1$，$C_2$分别表示$O_1$和$O_2$中的概念，$\bf{t_{1i}}$，$\bf{t_{2j}}$表示$C_1$，$C_2$中两个单词$t_{1i}$,$t_{2j}$的向量表示，${{w_i}}$ 表示${t_{1i}}$在概念$C_1$中的权重，计算方式如公式\label{equ:8}：
\begin{equation}
{w_i} = \frac{{TFIDF({t_{1i}})}}{{\sum\limits_{l = 1}^n {TFIDF({t_{1l}})} }}
\label{equ:8}
\end{equation}
这里$n$表示概念$C_1$所包含的单词数，$TFIDF( \cdot )$表示每个单词的TFIDF函数值。

\indent 因为$\bf{t_{1i}}$，$\bf{t_{2j}}$向量包含了单词丰富的上下文信息，所以基于语义嵌入的TFIDF策略能够更好的反映概念之间字面意思的相似情况，与字符串匹配相比，可以发现更多的概念同义词匹配。一方面，基于TF-IDF策略的改进算法依赖于词嵌入的质量，词嵌入的质量影响匹配的效果，因此这里采采用了高质量的预训练向量来保证基于上下文视角的匹配效果（参见第5.2节）。另一方面，使用基于其他视角的预训练模型生成的匹配来评估基于上下文视角所得匹配的质量（参见第4.2.3节）。

\noindent \textbf{2.基于结构视角的预训练模型}

\indent 现有的利用表示学习的本体匹配方法侧重于采用概念的名称，标签，定义等术语信息来学习的概念的词向量表示，却并没有充分利用本体中的结构关系，而有效的利用本体的结构信息，有助于提升本体匹配的效果。所以，本节将从结构视角构建表示学习模型来实现本体的匹配。首先假设由字符串等价或其同义词替换生成的待匹配对是正确的，将获得的待匹配对作为结构视角的表示学习模型模型训练的数据集，然后本文定义了基于交叉熵的损失函数来构建表示学习模型来得到概念的向量表示。损失函数定义如公式\label{equ:2}：
\begin{equation}
{l_{SE}} =  - \sum\limits_{({C_1},{C_2}, \equiv ,1.0) \in M} {\log {f_{SE}}({C_1},{C_2})}  - \sum\limits_{({C_1}^\prime ,{C_2}^\prime , \equiv ,1.0) \in M'} {\log {f_{SE}}({C_1}^\prime ,{C_2}^\prime )} )
\label{equ:2}
\end{equation}
其中$M$表示根据假设得到的待匹配对$\{ ({C_1},{C_2}, \equiv ,1.0)\}$构成的正例数据集，${M'}$表示负例数据集，为生成负例训练集${M'}$，MultiOM 采用了新的负采用技术（参见第三段）。通过对每个正例${({C_1},{C_2}, \equiv ,1.0) \in M}$，根据正太分布函数得到的概率在候选集中，采用新的负采样策略（参见第三段）替换$C_1$ 或$C_2$得到负采样$ {({C_1}^\prime ,{C_2} , \equiv ,1.0) \in M'}$或$ {({C_1} ,{C_2}^\prime , \equiv ,1.0) \in M'}$。$ {{f_{SE}}({C_1},{C_2})}$表示由公式\label{equ:3}定义的计算待匹配对得分的评分函数，其中$\bf{{C_1},{C_2} \in {R^d}}$表示两个本体$O_1$和$O_2$ 中概念${C_1}$，${C_2}$的d维向量表示；${\left\|  \cdot  \right\|_{\rm{2}}}$ 表示向量的2范数，这样设计的目的是对于两个相似的概念${C_1}$，${C_2}$，希望其得分$ {{f_{SE}}({C_1},{C_2})}$尽可能大，相反，对于不相似的概念对，希望其得分尽可能小。基于TransE的距离转移的思想，将评分函数$ {{f_{SE}}({C_1},{C_2})}$ 定义如公式\label{equ:3}：
\begin{equation}
{f_{SE}}({C_1},{C_2}){\rm{ = 2}} \cdot \frac{{\rm{1}}}{{{\rm{1 + }}{e^{({{\left\| {{C_1} - {C_2}} \right\|}_2})}}}}
\label{equ:3}
\end{equation}
以下详细介绍本文提出的一种不同于TransE以及TransH等表示学习模型从所有概念中进行采样替换的负采样方式，本文的负采样方式充分利用了本体的结构关系，具体方式如下，被替换概念的候选集由所有与该概念不构成SubClassOf，PartOf关系的概念构成，并且替换时若候选集集中存在与该概念构成disjointwith关系的其他概念，则优先将其作为采样对象。例如：以医学本体中的以SubClassOf关系为例，对于任意匹配对${({C_1},{C_2}, \equiv ,1.0) \in M}$，若$C_1$ 或$C_2$ 存在SubClassOf关系，即$({C_i},SubClassOf,{C_1})$ 或$({C_j},SubClassOf,{C_2})$，其中${C_i} \in {O_1}$，${C_j} \in {O_2}$，则负采样时需保证对$C_1$负采样时，候选集中没有$C_i$，对$C_2$负采样时候选集中没有$C_j$。此外，若$C_1$或$C_2$存在disjointwith关系,即$({C_m},disjointwith,{C_1})$或$({C_n},disjointwith,{C_2})$，则对$C_1$负采样时，优先采样$C_m$，对$C_2$负采样时，优先采样$C_n$。利用这些本体的结构信息作为约束改进负采样，使得生成的概念的向量表示包含了更多的本体结构信息，有助于提升MultiOM的匹配性能。

\noindent \textbf{3.基于外部资源视角的预训练模型}

\indent 受文献\cite{}中方法的启发，本节采用第三方本体作为外部资源来实现两个本体之间的连接，现实应用中存在很多不同但存在交叉的本体（即两个本体中包含多个相同的概念），如：MA，NCI与FMA这三个本体中存在很多共有的概念和关系，FMA，NCI和SNOMED-CT这三个本体中，也有很多共有的概念和关系，以上两个例子也是本文实验部分的本体匹配数据集。与字典或上下文信息等外部资源相比，第三方本体作为外部资源可以提供更多的结构信息，这有助于改进本体匹配的质量\cite{}。 然而，现有的利用第三方本体的方法主要是基于字符串的字面信息来实现匹配，这导致无法通过第三方本体结构信息挖掘更多的待匹配本体之间的语义信息，不利于本体匹配模型性能的提升\cite{}。因此，这里将第三方本体作为外部资源，使用表示学习技术，进一步通过第三方本体的结构信息来提升本体匹配的效果。

\begin{figure}[!h]
\centering
\includegraphics[scale=0.6]{model.png}
\caption{左：现有的利用第三方本体的模型框架。右图：本文使用第三方本体的表示学习模型框架}
\label{fig:model}
\end{figure}

\indent 如图\ref{fig:model}显示了现有的第三方本体类方法的模型框架到本文利用第三方本体作为外部信息的表示学习模型框架的演变。这里，采用$C \in {R^d}$向量来表示概念$C$，因为存在本体交叉的现象，所以不妨假设存在一些概念对$({C_1},{C_2})$，并且这些概念或它们在$O_1$，$O_2$中的同义词出现在第三方本体$O_3$中，例如图中MA，NCI，FMA中都有“Forelimb”，并且“Forelimb”与“Upper limb”等都是同义词。将以上这样的三元组记为$({C_1},{C_2},{C_3})$。 对于不同本体中的概念，这里将以上这些构成同义词的三元组$({C_1},{C_2},{C_3})$作为模型的训练数据集，基于TransR的思想，不同本体的概念在不同的向量空间中，引入两个映射矩阵实现不同本体概念的向量空间之间的映射，设计表示学习模型，模型损失函数定义如公式\label{equ:4}；
\begin{equation}
{l_{RE}} =  - \sum\limits_{({C_1},{C_2},{C_3}) \in \gamma } {\log {f_{RE}}({C_1},{C_2},{C_3}))}  - \sum\limits_{({C_1}^\prime ,{C_2}^\prime ,{C_3}) \in \gamma '} {\log {f_{RE}}({C_1}^\prime ,{C_2}^\prime ,{C_3}))} )
\label{equ:4}
\end{equation}
\indent 这里$\gamma$表示由同义词三元组$({C_1},{C_2},{C_3})$所构成的集合，${\gamma '}$表示随机替换$C_1$或者$C_2$后所构成的三元组集合$\{ ({C_1}^\prime ,{C_2}^\prime ,{C_3})\}$，${{f_{RE}}({C_1},{C_2},{C_3})}$表示等式\label{equ:5}所示用于计算投影后概念间相似度的得分函数，这里，$\bf{{C_1}},\bf{{C_2}},\bf{{C_3} \in {R^d}}$分别表示${O_1},{O_2},{O_3}$中概念${C_1},{C_2},{C_3}$的d维连续向量表示。${M_{13}}$ 和${M_{23}}$ 分别表示将概念${C_1},{C_2}$投影到第三方本体$O_3$中的映射矩阵，旨在实现将$O_1$和$O_2$中相似的概念投影到$O_3$中与他们相似的概念附近，相反，不相似的概念应该存在一定的距离，语义相反的词距离尽可能远。
\begin{equation}
{f_{RE}}({C_1},{C_2},{C_3}) = {\rm{2}} \cdot \frac{1}{{1 + {e^{({{\left\| {{C_1} * {M_{13}} - {C_3}} \right\|}_2} + {{\left\| {{C_2} * {M_{23}} - {C_3}} \right\|}_2})}}}}
\label{equ:5}
\end{equation}
\indent 两个映射矩阵非常重要，实现了不同本体间的语义关联，为了获得更好的映射矩阵，我们保持$O_3$中概念的向量表示不变，这样有助于缩减模型需学习的参数量，只更新两个映射矩阵以及$ {O_1},{O_2}$中概念的参数。这里需先将$O_3$中的结构关系作为数据集，利用预训练模型得到$O_3$中概念的向量表示，来保证$O_3$中概念的表示已包含其结构信息。因现有的预训练模型生成的表示稀疏性不好，无法体现$O_3$本体中的结构信息，所以，本文设计了如下的损失函数来构建表示学习模型来获得更好的第三方本体的概念表示，如公式\label{equ:6}：
\begin{equation}
{l_{RE}} =  - \sum\limits_{({C_{31}},r,{C_{32}}) \in \lambda } {\log {f_r}({C_{31}},{C_{32}}))}  - \sum\limits_{({C_1}^\prime ,r,{C_3}) \in \lambda '} {\log {f_r}({C_{31}}^\prime ,{C_{32}}^\prime ))} )
\label{equ:6}
\end{equation}
其中，评分函数${{f_r}}$如公式\label{equ:7}所示：
\begin{equation}
{f_r}({C_{31}},{C_{32}}) = 2 \cdot \frac{1}{{1 + {e^{^{({{\left\| {{C_{31}} - {C_{32}}} \right\|}_2} - a)}}}}}
\label{equ:7}
\end{equation}
这里$\lambda$表示由$O_3$中的结构关系三元组$({C_{31}},SubClassOf,{C_{32}})$，$({C_{31}},PartOf,{C_{32}})$构成的集合，${\lambda '}$是随机替换$ {C_{31}}$或${C_{32}}$得到的负例样本的集合，得分函数${f_r}({C_{31}},{C_{32}})$表示在关系$r$中两个满足关系$r$的概念${C_{31}}$和${C_{32}}$之间的得分，$\bf{{C_{31}}}$，$\bf{{C_{32}}}$对应${C_{31}}$和${C_{32}}$的向量表示，值得注意额是这里的SubClassOf以及PartOf 关系并不是等价关系，所以，我们已利用超参数$a$来控制概念向量之间的语义距离。

\noindent \textbf{4.多视角模型的匹配算法}

\indent 在得到不同视角模型获得的匹配后，最终需要将它们进行组合。一个简单的策略是从这些模块中收集所有匹配，并用一个阈值或稳定的结合算法进行过滤\cite{}。尽管这种策略最终可以获得很多正确匹配，但也可能引入许多错误的匹配以及容易忽略一对多，多对一或多对多的匹配\cite{}。因此，我们提出了一种基于相互评估的组合策略。

\indent 简记OM-L，OM-S，OM-R表示基于上下文模块，基于结构模块和基于外部资源模块各自生成的匹配。具体过程如下实现。

\indent Step1：通过相似度取极大来合并来自OM-S和OM-R的匹配。其合并结果被标记为OM-SR。

\indent Step2：根据相应的阈值${\delta_1}$和${\delta_2}$选择OM-L和OM-SR的可靠匹配（相似度高于阈值即为可靠匹配）。

\indent Step3：对可靠匹配进行互评。例如，如果一个可靠匹配属于OM-L，且在OM-SR中的相似性低于阈值${\delta_3}$，则将其删除；同样一个可靠匹配属于OM-SR，且其在OM-L中的相似性低于阈值${\delta_4}$，则删除此匹配。

\indent Step4：对来自OM-L和OM-SR的匹配取并，生成最终匹配。

\section{基于HORD算法对MultiOM的改进模型MultiOM+HORD}
{\songti\xiaosihao MuitiOM模型基于TransE，TransR构建表示学习模型，基于这些表示学习模型得到的概念或者关系的表示非常依赖模型的参数。为进一步获得更好的参数从而改善待匹配概念的表示并提高MuitiOM本体匹配的效果，本节考虑采用HORD算法对MuitiOM中的表示学习的超参数进行优化。

\indent HORD算法较贝叶斯类超参数优化方法，可以更加高效的同时优化整数或连续实数类超参数。MuitiOM中表示学习模型的超参数包括训练轮数（Epoch），学习率，向量维数，批次数（nbatchs）,合页损失中的$\gamma$的大小，以及负采样率，这些参数既包含整数也包含连续数，另外，模型本身的参数量较大，所以HORD 算法是适合用于实现MuitiOM模型超参数优化的方法。

\indent MuitiOM中的表示学习模型的超参数包括训练轮数（Epoch），学习率，向量维数，批次数（nbatchs）,合页损失中的$\gamma$的大小，以及负采样率。为降低参数调节的计算量，提升模型效率，本文未对以上六种参数都进行优化。首先，MuitiOM中的表示学习模型的参数规模由向量维数，以及负采样率决定，而参数旨在表示本体中概念的语义关系，我们可以根据本体所包含概念的多少，大致确定向量的维数，以及负采样率，保证可以适用于匹配任务即可。所以，MuitiOM中未对这两个参数进行调节；其次，训练轮数（Epoch）根据人工经验得到的值为1000，该参数决定了模型学习时间的长短，为保证参数优化后，可以提升MuitiOM的的效率，我们将其固定为500，通过调节剩余的三个参数，以期望以更高的效率获得当前或优于当前的匹配效果。}\\

\subsection{}
  {\songti\xiaosihao
  \begin{figure}[!h]
\centering
\includegraphics[scale=0.6]{framework.png}
\caption{本文算法框架}
\label{fig:framework}
\end{figure}

\indent 为实现MuitiOM中的表示学习模型的学习率，批次数（nbatchs）,合页损失中的$\gamma$的大小这三类参数的优化，进一步提升MuitiOM的性能，本文提出了如图\ref{fig:framework}的算法框架。因基于上下文视角的预训练模型采用训练好的词向量，并没有涉及参数优化，所以采用HORD算法对基于外部资源视角的预训练模型以及基于结构视角的模型基于学习率，批次数（nbatchs）,合页损失中的$\gamma$的大小进行参数优化。}\\
  \subsection{本体匹配评估指标}
  {\songti\xiaosihao 待完善}\\

\section{基于HORD算法的几类表示学习模型超参数优化的评估结果}
    {\songti\xiaosihao 待完善}\\
    \subsection{实验设置}
        {\songti\xiaosihao 待完善}\\
    \subsection{预测结果与分析}
        {\songti\xiaosihao 待完善}\\
\section{基于多视角的生物医学本体匹配模型--MuitiOM的评估结果}
    {\songti\xiaosihao 待完善}\\
    \subsection{实验设置}
        {\songti\xiaosihao 待完善}\\
    \subsection{本体匹配结果与分析}
        {\songti\xiaosihao 待完善}\\
\section{基于BERT模型和HORD算法对MuitiOM的改进的评估结果}
    {\songti\xiaosihao 待完善}\\
    \subsection{实验设置}
        {\songti\xiaosihao 待完善}\\
    \subsection{本体匹配结果与分析}
        {\songti\xiaosihao 待完善}\\

\ifx\allfiles\undefined
\end{document}
\fi
