\ifx\allfiles\undefined
\documentclass[a4paper,12.15pt,CJK,oneside]{article}
\begin{document}
%\pagestyle{plain}
\else
\fi
\chapter{相关工作}
\thispagestyle{fancy} \fancyhead[L]{\songti \wuhao
重庆师范大学硕士学位论文}\fancyhead[R]{\songti \wuhao 2~~相关工作}
{\songti\xiaosihao 自20世纪末，国内外学者便开始了对本体匹配的研究，如今，已有大量用于解决本体异构问题的本体匹配方法，这些方法大致可以分为两类，基于特征工程的方法以及基于深度学习的方法。本章主要对现有的本体匹配方法及知识图谱表示学习模型，“黑箱”优化问题，HORD算法进行阐述。
\\
\section{基于统计学习的本体匹配方法}
\indent 基于特征工程的方法有2001年Jayant Madhavan等人\cite{cupid}提出的使用字符串和词典两种技术，利用本体结构信息，而未采用实例信息，通过键值约束，引用约束等信息对Schema进行匹配的方法Cupid；2002年，Sergey Melnik等人\cite{sf}提出了一种面向一般的图的匹配算法Similarity Flooding(SF)；Fausto Oiuachiglia等人\cite{smatch}提出的一种采用多种自然语言处理技术，主要面向概念层次结构的本体的方法S-Match；2005年，P.Mitra等人\cite{omen}提出的利用贝叶斯网络，使用规则获取本体结构信息来生成贝叶斯网络条件概率表，通过阈值筛选匹配的的OMEN方法；2008年，有W.Hu等人\cite{falconao}的方法和Wang等人的方法，W.Hu等人\cite{falconao}提出了一种基于概念与属性之间的亲密度来分割本体，将大规模本体匹配任务转化为多个小规模匹配从而可以完成大规模本体匹配任务的本体匹配方法Falcon-AO；P.Wang等人\cite{lily}提出了一种能够处理大型本体映射任务，并且嵌入了匹配修复方法来进一步提高匹配的质量本体匹配方法Lily，2009年，J.Li等人\cite{rimom}提出了一种基于贝叶斯理论，将本体匹配中的问题转换成经验风险最小化问题，先基于本体的概念的语义特征计算相似度，再对相似度聚类得到初始匹配，最后基于当前匹配挖掘潜在匹配的采用多策略的本体映射方法RiMOM；2011年，M.Niepert等人\cite{codi}提出的采用概率模型和马尔可夫逻辑网相结合的方式来评估概念对的相似度的CODI方法；Mao等人\cite{mao}的方法，将本体匹配任务转化为一个二分类的问题。通过生成各种与领域无关的与特征来描述实体的特征，并采用包含正例与负例概念对的训练集训练支持向量机分类器，最后用训练好的支持向量机来预测正确匹配。2012年，D.Ngo等人\cite{yam}提出了一个可以自行配置、灵活并且可扩展，即可根据本体匹配任务规模的大小选取不同匹配策略的的本体匹配方法YAM++；S.Albagli等人\cite{imatch} 提出的基于马尔可夫网络，采用近似推理算法来计算相似度的交互式本体匹配方法iMatch；2013年Daniel Faria等人\cite{aml}提出了一种基于元素级匹配，以外部资源为背景知识，自动化的，且有效利用本体概念的字符串和结构信息的本体匹配方法AML；2014年，Djeddi 等人\cite{xmap}提出的综合了概念的术语信息，结构信息，和上下文信息，然后通过相似度聚合得到匹配对的XMAP方法；以及2016年Gulic等人的方法\cite{gulie} 和2018年M.Zhao等人的工作\cite{fca}，Gulic等人\cite{gulie}提出的在权重更新过程中引入新概念，基于本体结构特征的一对一匹配系统CroMatcher\cite{gulie}，2018年M.Zhao提出了一种利用形式概念分析逐步来建模本体的各类信息，在大型医学本体的映射任务上表现不错的本体匹配方法FCA-Map\cite{fca}。以上方法都是从本体中概念的术语信息，结构相似性，结构约束，概念属性等信息中提取特征，基于所选特征，再构建各种图模型，概率模型等各种不同的模型，通过极大化，极小化，求均值，加权求和等方式来获得概念之间的相似度，通过相似度的高低来选取合适的匹配。而还有少许方法使用一些逻辑将本体匹配转化为可满足性问题，通过约束，来选择满足条件的匹配。这类工作有2004年，Jerome Euzenat等人\cite{ola}提出的一种综合使用了字符串距离和词汇距离来比较计算两个URIref的相似度，针对OWL-Lite所表示的本体进行匹配的方法OLA；2011年，E.Jim'enez-Ruiz等人\cite{lm}提出了一种具有高扩展性，支持推理并且能处理不协调本体匹配情况的方法LogMap。
\\
\section{基于深度学习的本体匹配方法}
\indent 虽然以上给定的基于特征工程的方法，在其给定的，与其方法特点相对应的数据集中可以取得比较好的效果，但在实际的本体匹配任务中，对于特征工程类本体匹配方法，人工的特征工程很难找到合适的特征，常常是非常耗时的工作\cite{deep}。而基于深度学习的本体匹配方法，通常不需要对特征进行人工选择，而是通过对模型参数的调节，实现模型自动选择特征，基于深度学习的本体匹配方法不仅证明了模型自行选择特征是有效的，并且可以获得更适合的特征，提升本体匹配的效果。到目前为止，深度学习的医学本体匹配方法有：2005年，Chortaras等人\cite{chor}提出的一种基于递归神经网络模型利用本体实例学习本体概念之间相似性的本体自动对齐方法；2006年Hariri等人\cite{hari}提出的基于多种基于特征工程的基础匹配器得到的匹配结果构建标签数据集，采用监督学习构建神经网络模型进行灵敏度分析，选出合适的匹配器，最后再通过神经网络模型学习各个基础匹配器的权重的方法；2007年，Merlin等人\cite{merl}提出了一种灵活、可扩展的本体映射和集成工具，通过一个前馈神经网络将几种匹配算法结合起来本体匹配方法X-SOM；2008年Huang 等人\cite{huang}提出了一种将从实例，概念字符串，概念名等信息中获得的特征，采用神经网络模型学习各个特征权重来实现匹配任务的深度学习方法；Gracia等人\cite{grac}提出了一种基于模式的神经网络本体匹配方法CIDER，以及在CIDER基础上实现跨语言匹配的CIDER-CL；Ichise等人\cite{ichi}提出了一种综合多个概念相似性度量来解决本体映射问题的深度学习模型；2010年Peng等人\cite{peng}提出了一种重在识别对于同一元素存在多种关系映射的神经网络模型OMNN；Esposito等人\cite{espo}提出了一种通过结构验证和聚合来筛选匹配，支持多种机器学习技术，应用神经网络模型的自动本体匹配系统MoTo；2012年，Rubiolo等人\cite{rubi}提出了一种将基于代理模型的分类器与基于人工神经网络分类器相结合的本体匹配方法；2013年Djeddi等人\cite{dieddi}提出的一种通过将多个基于统计学习的不同基础匹配器得到的语义相似度，采用神经网络模型训练得到各个相似度的权重，借助权重将多个相似性度组合成最终相似度的方法XMAP++；Shenoy等人\cite{shen}提出了一种基于元数据和实例信息计算不同类型相似性，并使用神经网络模型融合不同相似度的本体匹配方法。

\indent 随着表示学习的发展，表示学习展现了其对语义特征提取的优势。利用表示学习的方法有2014年Zhang等人\cite{zhang}的方法是第一个使用词向量表示来解决本体匹配问题的方式，他们用word2vec\cite{word}来训练维基百科预料，通过语义转换来补充词汇的信息，通过最大相似度来筛选合适的匹配。2015年Xiang 等人\cite{xiang} 提出的是基于堆叠自动编码器的实体表示学习算法，他们利用本体的自身信息，采用定点算法计算实体的相似性，最后使用stable marriage算法进行匹配。2017年Qiu等人\cite{qiu}提出的采用表示学习方法，通过词向量来等价表示实体各类信息的无监督自动编码网络；2018 年Wang等人\cite{ontoemma}提出的是一种在本体匹配的监督学习框架Ontoemma，采用本上下文作为外部资源，来提高预测准确性的神经结构模型。对于给定的一个候选对，首先对待匹配概念对进行编码，然后进入一个多层感知机网络，最后训练模型并预测这两个实体等价的可能性。然而监督学习类方法在本体匹配任务中，依然无法克服类别不均衡的问题。而更多的是无监督的方法。Prodromos Kolyvakis 等人的Deepalignmeent方法\cite{deep}主要利用字典作为外部资源来改进预训练的词向量，从而得到更适合本体匹配任务的实体向量表示；SCBOW\cite{scbow}与Deepalignmeent方法不同，利用字典和上下文同时作为外部资源，将实体嵌入高维欧几里得空间，采用一种新的短语改造策略，将语义相似信息嵌入到预训练好的词向量的字段中，提出了一种基于降噪自编码器的离群点检测机制的本体匹配方法。

\indent 虽然基于深度学习的本体匹配方法相比人工特征的方法，可以学到更加合适的特征，提升本体匹配的效果。但是，词向量无法准确表示概念的语义，而本体本身的结构信息也包含了大量的语义，忽略掉本体本身的结构信息会直接影响本体匹配方法的性能。同时，基于深度学习的本体匹配方法的性能依赖于模型所学特征，其常常使用随机向量或者基于文本的预训练模型来提取待匹配概念的特征或表示。但随机向量或者文本预训练模型得到的概念向量表示通常会将概念做为一个独立的单元忽略了概念之间的关联关系。 这导致其表示不符合概念在知识图谱中的表示。近年来知识图谱表示学习模型展现了其在知识表示的优势，但现有的工作证明，在知识图谱表示模型中，概念或者关系的表示非常依赖模型的参数。所以，如何在知识图谱表示学习中找到更好的参数从而改善待匹配概念的表示并提高本体匹配的效果也是现有深度学习类方法的问题之一。
\\
\section{知识图谱表示学习模型}
\\
\subsection{知识图谱表示学习}
{\songti\xiaosihao 深度学习技术作为人工智能领域的核心技术之一，被广泛应用于机器视觉，语音识别，自然语言处理等众多领域。深度学习实现了数据的分布式表示，采用稠密的低维实值向量来表示数据的含义，而表示学习正是实现这类表示的方法。表示学习，是对数据表征的学习，它从数据中提取有用的信息，以便于构建机器学习模型\cite{rl}，表示学习的发展也影响了知识图谱的表示形式，从而产生了知识图谱表示学习（也可称为知识表示学习）。知识图谱表示学习旨在将知识图谱中的实体和关系借助模型的学习表示成低维实值向量，进而通过向量之间的关系来描述实体之间的语义关联。}
\\
\subsection{知识图谱表示学习模型}
{\songti\xiaosihao 本节重点介绍本文研究内容中涉及的若干种知识图谱表示学习方法。按是否引入额外信息为区分标准，知识图谱表示学习方法可分为仅利用三元组的方法和引入额外信息的方法\cite{wangquan}，表示学习模型众多，本节主要介绍本文用到的仅利用三元组的方法，仅利用三元组的方法又包括基于转移距离的方法和基于语义匹配的方法\cite{wangquan}，基于转移距离的方法有TransE\cite{transe},TransH\cite{transh},TransR\cite{transr},TransD\cite{transd} 等，基于语义匹配的方法有DistMult\cite{dismult},ComplEx\cite{compiex},ConvE\cite{conve}等。接下来以TransE,TransH,TransR为例，进行介绍。

\noindent \textbf{TransE}

\begin{figure}[!h]
\centering
\includegraphics[scale=0.35]{TransE.png}
\caption{“TransE”模型示意图}
\label{fig:transe}
\end{figure}

TransE是由Bordes等人提出的将任意知识图谱三元组$(h,r,t)$中的关系$r$看作实体间的转移向量，实现头实体向量$h$和尾实体向量$t$之间的转移操作的方法。如图\ref{fig:transe}所示：对于三元组$(h,r,t)$，模型尽可能使得$h + r \approx t$，所以模型的评分函数定义为：
\[{f_{TransE}}(h,r,t) =  \left\| {h + r - t} \right\|\]
该函数意在表示$h + r$与$t$之间的距离，而这里的$\left\|  \cdot  \right\|$指1范数或2范数距离，即${\left\| {h + r - t} \right\|_1} = \sum\nolimits_{i = 1}^k {\left| {{h_i} + {r_i} - {t_i}} \right|}$，${\left\| {h + r - t} \right\|_2} = \sqrt {\sum\nolimits_{i = 1}^k {{{\left| {{h_i} + {r_i} - {t_i}} \right|}^2}} }$，距离越小，则该三元组成立的可能性就越大，置信度越高。

\indent 构建模型过程中，为增强模型对于正确与错误三元组的区分能力，TransE采用了合页损失函数，定义了如下的损失函数：
\[L = \sum\limits_{(h,r,t) \in \xi } {\sum\limits_{(h',r',t') \in \xi '} {\max (0,{f_{TransE}}(h,r,t) + \gamma  - {f_{TransE}}(h',r',t'))} }\]

其中，$\xi$是所有正确三元组$(h,r,t)$的集合，${\xi '}$为错误三元组${(h',r',t')}$的集合，两者构成模型的训练数据集， $\gamma$为正例评分与负例评分之间的间隔，该函数意在使得正例得分尽可能小，负例得分尽可能大，同时使得两者之间的间隔要大于$\gamma$，从而提升模型的判别能力。

\indent 负样本是正样本中的每个三元组$(h,r,t)$，任意替换头实体或者尾实体（不同时换）后构成的集合，符号语言为：$\xi ' = \left\{ {(h',r,t)|h' \in E} \right\} \cup \left\{ {(h,r,t')|t' \in E} \right\}$，这里E表示所有实体的集合。

\indent 训练模型时，TransE需要对以下参数进行设定，训练轮数（Epoch），向量维数，批次数（nbatchs）,$\gamma$的大小，以及负采样率，即对每个正样本需构成的负样本数量，然后利用损失函数计算损失，采用随机梯度下降算法更新其参数。

\begin{figure}[!h]
\centering
\includegraphics[scale=0.45]{Trans.png}
\caption{“TransH”和“TransR”模型示意图}
\label{fig:trans}
\end{figure}

\indent TransH,TransR,TransD都是基于TransE的改进模型，与TransE拥有类似的损失函数及训练方式。而TransE模型在一对多或多对一的复杂三元组关系下，效果不理想，例如：给定任意一个头实体会对应多个尾实体的关系$r$，则存在$(h,r,{t_1})$和$(h,r,{t_2})$，因为$h + r \approx {t_1}$，$h + r \approx {t_2}$，所以有${t_1} \approx {t_2}$，这便无法区分${t_1}$和${t_1}$。为解决这一问题，TransH,TransR,TransD分别做出了不同程度的改进，此外，TransH将TransE的负采样方式改进为采用根据概率分布进行负采样的方式。下面逐一介绍这些模型的评分函数构造的思想。

\noindent \textbf{TransH}

\indent Wang等人提出了TransH模型，为解决TransE在一对多或多对一的三元组关系下，效果不理想的问题，TransH规定实体的表示由不同的关系所决定，不同的关系对应不同的超平面，如图\ref{fig:trans}，对任意三元组$(h,r,t)$，该模型将头实体$h$和尾实体$t$沿关系$r$所在超平面的法向量$ {{\rm{w}}_r}$投影到关系$r$所在平面中，所以：
\[{h_r} = h - {\rm{w}}_r^Th{{\rm{w}}_r},{t_r} = t - {\rm{w}}_r^Tt{{\rm{w}}_r}\]
因此，TransH采用与TransE类似的思想，构建评分函数：
\[{f_{TransH}}(h,r,t) =  \left\| {h_r + r - t_r} \right\|\]

\noindent \textbf{TransR}

\indent TransH虽然实现了不同关系，实体的表示，但依然假设实体与关系处于同一向量空间，这也影响了TransH模型的表示能力，为进一步弱化该问题的影响，Lin等人提出了TransR模型，TransR认为实体和关系处于不同空间，在映射矩阵$M_r$作用下，将实体投影到关系空间中，从而有：
\[{h_r} = {M_r}h,{t_r} = {M_r}t\]}

\section{超参数优化与HORD算法}
\\
\subsection{超参数优化问题}
{\songti\xiaosihao 近年来，各类机器学习，深度学习技术被广泛应用于各种与人工智能相关的应用中，这类模型中参数规模较大，只知道其输入与输出没有显式表达式的模型就是黑箱函数。通常黑箱函数的性能会受到模型超参数的制约，所以通过算法进一步优化模型超参数来提升模型性能具有重要现实意义，对这样的黑箱超参数优化问题可以理解为将需要配置$d$个超参数看作一组向量$x \in {R^d}$，模型本身可以看作关于$x$ 的“黑箱”函数$f(x)$，超参数优化可以看作解决以下的优化问题，在训练集${Z_{train}}$ 和测试集${Z_{val}}$ 中，函数$f(x)$ 将$d$个可配置超参数中的超参数选择$x$ 映射到具有学习参数θ的深度学习算法的验证误差。
\[\begin{array}{l}\large
\begin{array}{*{20}{c}}
{\mathop {\min }\limits_{x \in {R^d}} }&{f(x,\theta ;{Z_{val}})}
\end{array}\\
\begin{array}{*{20}{c}}
{s.t.}&{\theta  = \arg \min }
\end{array}f(x,\theta ;{Z_{train}})
\end{array}\]
\subsection{HORD算法}
\indent 超参数调节较为流行的一类方法是基于贝叶斯的优化方法，例如：基于高斯过程的方法以及基于树结构的方法等，与这类算法相比，HORD算法能够同时优化连续超参数与整数超参数，并且可以以更少的估值次数找到较好的验证误差。表示学习模型的参数包括训练轮数（epoch），向量维数，批次数（nbatchs）,$\gamma$的大小，以及负采样率，学习率，既包含整数也包含连续实数，并且训练轮数常常设置比较大，无论是人工经验或者基于贝叶斯类优化方法调参都是一项十分耗时的任务。而HORD算法较可以较少的估值次数很好的实现既包含整数也包含连续数的超参数优化。

\indent $f(x)$非常复杂并且没有显式表达式，所以求解大规模参数的问题5效率不高。为提高优化参数的效率，HORD算法\cite{hord}利用径向基函数插值模型构建响应面模型，通过动态超参数坐标搜索来找到整数或连续值超参数的近似最优配置。已知可行的$n$种超参数配置${x_i}(i = 1,2,3, \ldots ,n)$，验证误差${f_i}(i = 1,2,3, \ldots ,n)$，这里$f_i=f(x_i)$，径向基函数响应面模型具体构造如下：
\[{S_n}(x) = \sum\limits_{i = 1}^n {{\lambda _i}\phi (\left\| {x - {x_i}} \right\|)}  + p(x)\]

这里$\phi (r) = {r^3}$，$p(x) = {b^T}x + a$，$b = {\left[ {{b_1}, \ldots ,{b_d}} \right]^T} \in {R^d}$，$a \in R$是该插值模型的参数，通过求解以下线性系统得到:
\[\left[ {\begin{array}{*{20}{c}}
\Phi &P\\
{{P^T}}&0
\end{array}} \right]\left[ {\begin{array}{*{20}{c}}
\lambda \\
c
\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}
F\\
0
\end{array}} \right]\]

其中，$\Phi  \in {R^{n \times n}}$，${\Phi _{i,j}} = \phi (\left\| {x - {x_i}} \right\|)$，$i,j = 1, \ldots n$，
\[P = \left[ {\begin{array}{*{20}{c}}
{{x_1}^T}&{}&1\\
{}& \vdots &{}\\
{{x_n}^T}&{}&1
\end{array}} \right]\lambda  = \left[ {\begin{array}{*{20}{c}}
{{\lambda _1}}\\
 \vdots \\
{{\lambda _n}}
\end{array}} \right]c = \left[ {\begin{array}{*{20}{c}}
{{b_1}}\\
 \vdots \\
{{b_k}}\\
a
\end{array}} \right]F = \left[ {\begin{array}{*{20}{c}}
{f({x_1})}\\
 \vdots \\
{f({x_n})}
\end{array}} \right]\]

HORD算法的主要思想包含以下几步：第1步，借助径向基插值函数构建响应面模型；然后设置待优化参数的范围与初始采样点个数以及最大迭代步数，通过拉丁超立方体采样得到多组变量与函数值的初始点对${{\rm A}_{{n_0}}} = \{ ({x_i},f({x_i}))\} _{i = 1}^{{n_0}}$；第2步，通过这些点对${{\rm{A}}_{{n_0}}}$，来得到响应面模型的解析式；第3步，借助当前响应面模型，并结合动态坐标扰动\cite{dycors}，求得最优值点${x_ * }$，同时计算出对应的函数值$f({x_ * })$；第4步，将当前最优点对并入初始点对${{\rm{A}}_{{n_0}}} = {{\rm{A}}_{{n_0}}} \cup \{ ({x_*},f({x_*}))\} $，第5步，在未达到最大步数时，重复步骤2至步骤4。
}\\
\ifx\allfiles\undefined
\end{document}
\fi
