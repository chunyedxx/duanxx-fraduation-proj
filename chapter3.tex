\ifx\allfiles\undefined
\documentclass[a4paper,12.15pt,CJK,oneside]{article}
\begin{document}
%\pagestyle{plain}
\else
\fi
\chapter{预备知识}
\thispagestyle{fancy} \fancyhead[L]{\songti \wuhao
重庆师范大学硕士学位论文}\fancyhead[R]{\songti \wuhao 3~预备知识}
 \songti\xiaosihao{本章主要介绍本文研究内容中所涉及的先验知识及相关概念，主要包括：知识图谱表示学习模型，“黑箱”优化问题，HORD算法，BERT模型}
 
\section{HORD算法}
\subsection{HORD算法}
{\songti\xiaosihao 今年来，深度学习技术被广泛应用于各种与人工智能相关的应用中，然而，深度学习模型的性能受到模型超参数的制约，而如何选择合适的参数其实是比较困难且耗时，因此如何实现深度学习超参数的自动优化是亟待解决的问题之一。深度学习的超参数优化是一类昂贵的“黑箱”全局优化问题，我们可以将需要配置$d$ 个超参数看作一组向量$x \in {R^d}$，深度学习模型可以看作关于$x$的“黑箱”函数$f(x)$，超参数优化可以看作解决以下的优化问题，在训练集${Z_{train}}$和测试集${Z_{val}}$中，函数$f(x)$将$d$个可配置超参数中的超参数选择$x$映射到具有学习参数θ的深度学习算法的验证误差。
\[\begin{array}{l}
\begin{array}{*{20}{c}}
{\mathop {\min }\limits_{x \in {R^d}} }&{f(x,\theta ;{Z_{val}})}
\end{array}\\
\begin{array}{*{20}{c}}
{s.t.}&{\theta  = \arg \min }
\end{array}f(x,\theta ;{Z_{train}})
\end{array}\]
\indent 因为$f(x)$是非常复杂没有显式表达式的函数，所以求解以上问题是非常困难的。为了解决这类优化问题，HORD算法利用径向基函数插值模型构建响应面模型，通过动态超参数坐标搜索来找到整数或连续值超参数的近似最优配置，所以该方法也是一种混合整数优化的方法。具体如下，对可行的$n$种超参数配置${x_i}(i = 1,2,3, \ldots ,n)$，验证误差${f_i}(i = 1,2,3, \ldots ,n)$，这里$f_i=f(x_i)$，构建如下响应面模型：
\[{S_n}(x) = \sum\limits_{i = 1}^n {{\lambda _i}\phi (\left\| {x - {x_i}} \right\|)}  + p(x)\]

这里$\phi (r) = {r^3}$，$p(x) = {b^T}x + a$，$b = {\left[ {{b_1}, \ldots ,{b_d}} \right]^T} \in {R^d}$，$a \in R$是该插值模型的参数，通过求解以下线性系统得到:
\[\left[ {\begin{array}{*{20}{c}}
\Phi &P\\
{{P^T}}&0
\end{array}} \right]\left[ {\begin{array}{*{20}{c}}
\lambda \\
c
\end{array}} \right] = \left[ {\begin{array}{*{20}{c}}
F\\
0
\end{array}} \right]\]

其中，$\Phi  \in {R^{n \times n}}$，${\Phi _{i,j}} = \phi (\left\| {x - {x_i}} \right\|)$，$i,j = 1, \ldots n$，
\[P = \left[ {\begin{array}{*{20}{c}}
{{x_1}^T}&{}&1\\
{}& \vdots &{}\\
{{x_n}^T}&{}&1
\end{array}} \right]\lambda  = \left[ {\begin{array}{*{20}{c}}
{{\lambda _1}}\\
 \vdots \\
{{\lambda _n}}
\end{array}} \right]c = \left[ {\begin{array}{*{20}{c}}
{{b_1}}\\
 \vdots \\
{{b_k}}\\
a
\end{array}} \right]F = \left[ {\begin{array}{*{20}{c}}
{f({x_1})}\\
 \vdots \\
{f({x_n})}
\end{array}} \right]\]

\indent 构建好响应面模型后，首先通过有限次的采样，得到多组变量与函数值的初始点对${{\rm A}_{{n_0}}} = \{ ({x_i},f({x_i}))\} _{i = 1}^{{n_0}}$，通过这些点对，来得到响应面模型的解析式；然后借助解析式求得当前响应面模型的最优值点${x_ * }$，同时计算出对应的函数值$f({x_ * })$；再将当前最优点对并入初始点对   ${{\rm A}_{{n_0}}} \cup \{ ({x_ * },f({x_ * }))\} $，重复以上步骤，直到满足终止条件。}\\

 
\section{知识图谱表示学习模型}
\subsection{知识图谱表示学习}
{\songti\xiaosihao 深度学习技术作为人工智能领域的核心技术之一，针对深度学习的相关的研究也层出不穷，众多性能良好的模型，如：全连接神经网络，卷积神经网络，循环神经网络，长短期记忆网络，递归神经网络等，被广泛应用于机器视觉，语音识别，自然语言处理等众多领域。深度学习实现了数据的分布式表示，采用稠密的低维实值向量来表示数据的含义，而表示学习正是实现这类表示的方法。表示学习，是对数据表征的学习，它从数据中提取有用的信息，以便于构建机器学习模型\cite{rl}，表示学习的发展也影响了知识图谱的表示形式，从而产生了知识图谱表示学习（也可称为知识表示学习）。知识图谱表示学习旨在对知识图谱中的实体和关系进通过模型的学习表示成低维实值向量，进而通过向量之间的关系来描述实体之间的语义关联。}
\\
\subsection{知识图谱表示学习模型}
{\songti\xiaosihao 本节重点介绍本文研究内容中涉及的若干种知识图谱表示学习方法。按是否引入额外信息为区分标准，知识图谱表示学习方法可分为仅利用知识图谱三元组的方法和引入知识图谱额外信息的方法\cite{}，本节主要介绍本文用到夫人仅利用三元组的方法，仅利用知识图谱三元组的方法又包括基于转移距离的方法和基于语义匹配的方法，基于转移距离的方法有TransE,TransH,TransR,TransD等。基于语义匹配的方法有DistMult,ComplEx,Conve等。接下来以TransE,TransH,TransR为例，进行介绍。

TransE

\begin{figure}[!h]
\centering
\includegraphics[scale=0.4]{TransE.png}
\caption{“TransE”模型示意图}
\label{fig:transe}
\end{figure}

TransE是由Bordes等人对每个三元组$(h,r,t)$将知识图谱中的关系$r$看作实体间的转移向量，实现头实体向量$h$和尾实体向量$t$之间的转移操作的方法。如图\ref{fig:transe}所示：对于每个三元组$(h,r,t)$，模型尽可能使得$h + r \approx t$，所以模型的评分函数定义为：
\[{f_{TransE}}(h,r,t) =  \left\| {h + r - t} \right\|\]
该函数意在表示$h + r$与$t$之间的距离，而这里的$\left\|  \cdot  \right\|$可以是1范数距离或者是2范数距离，即${\left\| {h + r - t} \right\|_1} = \sum\nolimits_{i = 1}^k {\left| {{h_i} + {r_i} - {t_i}} \right|}$，${\left\| {h + r - t} \right\|_2} = \sqrt {\sum\nolimits_{i = 1}^k {{{\left| {{h_i} + {r_i} - {t_i}} \right|}^2}} }$，距离越小，则该三元组成立的可能性就越大，置信度越高。

\indent 构建模型过程中，为增强模型对于正确与错误三元组的区分能力，TransE采用了合页损失函数，定义了如下的损失函数：
\[L = \sum\limits_{(h,r,t) \in \xi } {\sum\limits_{(h',r',t') \in \xi '} {\max (0,{f_{TransE}}(h,r,t) + \gamma  - {f_{TransE}}(h',r',t'))} } \]

其中，$\xi$是所有正确三元组$(h,r,t)$的集合，${\xi '}$为错误三元组${(h',r',t')}$的集合，两者便是模型的训练数据集， $\gamma$为正例评分与负例评分之间的间隔，该函数意在使得正例的得分尽可能小，负例的得分尽可能大，并且使得两者之间的间隔要大于$\gamma$，从而提升模型的判别能力。

\indent 负样本是正例样本中的每个三元组$(h,r,t)$，任意的替换头实体或者尾实体（不同时换）后构成的集合，符号语言为：$\xi ' = \left\{ {(h',r,t)|h' \in E} \right\} \cup \left\{ {(h,r,t')|t' \in E} \right\}$，这里E表示所有实体的集合，但这也会导致这样得到的负例也刚好为正例，而TransE认为这样的概率非常小，对模型效果影响也很小，就暂未考虑。

\indent 训练模型时，TransE需要对以下参数进行设定，训练轮数（Epoch），向量维数，批次数（nbatchs）,$\gamma$的大小，以及负采样率，即对每个正样本需构成的负样本数量，然后利用损失函数计算损失，采用随机梯度下降算法更新其参数。

\begin{figure}[!h]
\centering
\includegraphics[scale=0.5]{Trans.png}
\caption{“TransH”和“TransR”模型示意图}
\label{fig:trans}
\end{figure}

\indent TransH,TransR,TransD都是基于TransE的改进模型，与TransE拥有类似的损失函数及训练方式。而TransE模型在一对多或多对一的复杂三元组关系下，效果不理想，例如：给定一个一对多的关系$r$，即一个头实体会对应多个尾实体，取其中两个：$(h,r,{t_1})$和$(h,r,{t_2})$，在TransE中会有$h + r \approx {t_1}$，$h + r \approx {t_2}$，所以有${t_1} \approx {t_2}$，这样便无法区分${t_1}$和${t_1}$。为了解决这一问题，TransH,TransR,TransD分别提出了不同的评分函数，此外，TransH将TransE随机替换头实体或者尾实体的负采样方式改进为采用根据伯努利分布或正态分布进行负采样的方式。下面重点逐一介绍这些模型的评分函数构造的思想。

TransH

\indent 为了解决TransE在一对多或多对一的复杂三元组关系下，效果不理想的问题，Wang等人提出了TransH模型。TransH规定实体的表示由不同的关系所决定，不同的关系对应不同的超平面，实体的表示在同一超平面中，如图\ref{fig:trans}，对于任意三元组$(h,r,t)$，该模型将头实体$h$和尾实体$t$沿着关系$r$所在超平面的法向量$ {{\rm{w}}_r}$投影到关系$r$所在平面中，所以：
\[{h_r} = h - {\rm{w}}_r^Th{{\rm{w}}_r},{t_r} = t - {\rm{w}}_r^Tt{{\rm{w}}_r}\]
将实体映射到关系平面中后，TransH采用与TransE类似的思想，构建评分函数：
\[{f_{TransH}}(h,r,t) =  \left\| {h_r + r - t_r} \right\|\]

TransR

\indent TransH虽然实现了对不同的关系，实体有不同的表示，但该模型依然假设实体与关系处于同一向量空间中，这也影响了TransH模型的表示能力，为进一步弱化该问题的影响，Lin等人提出了TransR模型，TransR认为实体和关系处于不同空间，在映射矩阵$M_r$作用下，将实体投影到关系空间中，从而有：
\begin{equation}
\[{h_r} = {M_r}h,{t_r} = {M_r}t\]
\end{equation}
TransR的评分函数与损失函数以及训练方式TransH相同，这里就不介绍了。

其他模型

\indent 表示学习模型除了以上详细介绍的三种以外，还有很多其他模型，如：基于距离转移的TransD,TranSparse,TransA,TransG,KB2E等，基于语义匹配的RESCAL,DistMult,HolE,ComplEx等，这些方法都是通过不同的方式来构建三元组中实体与关系的关系，来构建模型，通过训练，来获得实体与关系的表示，从而应用到下游应用中。

}\\
\section{BERT模型}
        {\songti\xiaosihao 待完善}\\
\section{基于HORD算法的几类表示学习模型超参数优化}


\ifx\allfiles\undefined
\end{document}
\fi