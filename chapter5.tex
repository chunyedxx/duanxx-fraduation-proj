\ifx\allfiles\undefined
\documentclass[a4paper,12.15pt,CJK,oneside]{article}
\begin{document}
%\pagestyle{plain}
\else
\fi

\chapter{实验与评估}
\thispagestyle{fancy} \fancyhead[L]{\songti \wuhao
重庆师范大学硕士学位论文}\fancyhead[R]{\songti \wuhao 4~实验与评估}

\section{评估指标与数据集}

\subsection{数据集}
{\songti\xiaosihao 本节简要概述在本体匹配实验中使用的四种本体，其中两个本体FMA(Foundational Model of Anatomy)和MA(Adult Mouse
anatomical)分别表示解剖学本体和成年小鼠解剖学本体，二者都是纯粹的解剖学本体论，而另外两个SNOMED CT和NCI是涉及更广的生物医学本体，解剖学只是它们描述的一个子领域\cite{}。虽然这些本体的最新版本是可用的，但是本文在整个本体匹配任务中参考了出现在OAEI(Ontology Alignment Evaluation Initiative) 中的版本，以便在本体匹配系统之间进行比较。

\indent FMA(Foundational Model of Anatomy)：这是一个一直被更新的本体，自1994年由华盛顿大学开发并维护\cite{}，其目的是以机器可读的形式概念化人体的表型结构。

\indent MA(Adult Mouse Anatomical Dictionary)：该本体是描述成年小鼠解剖结构的结构的词汇表\cite{}。

\indent NCI(NCI Thesaurus)：该本体提供了癌症的标准词汇\cite{}，其解剖学子域描述人类自有的生物结构、液体和物质。

\indent SNOMED(SNOMED Clinical Terms)：该本体是一个系统化，组织化的机器可读的医学术语集合，提供临床文档和报告中使用的代码、术语、同义词和定义\cite{}。

\indent 以上本体的概念及对应匹配数量具体如图\ref{tab:dataset}：
\begin{table}[h]
\centering
\caption{本体概念数及对应匹配数量}
\label{tab:dataset}
\begin{tabular}{ccccc}
\hline
 源本体&概念数&目标本体&概念数&匹配数\\
\hline
    MA&2744&NCI&3304&1489\\
\hline
 FMA&3696&NCI&6488&2504\\
\hline
  FMA&10157&SNOMED&13412&7774\\
\hline
\end{tabular}
\end{table}
}
\subsection{评估指标}
  {\songti\xiaosihao
  OAEI为本体匹配提供了统一的评估标准，评估的指标主要是：匹配的准确率，召回率和F1值。为阐述以上三个指标含义，首先介绍以下概念：

  \indent      TP(True Positives) ：表示模型将正例预测为正例的数据条数；

  \indent      FP(False Positives)：表示模型将负例预测为正例的数据条数；

  \indent      FN(False Negatives)：表示模型将正例预测为负例的数据条数；

  \indent      TN(True Negatives) ：表示模型将负例预测为负例的数据条数；

\indent 所以，由以上四个参数值，可根据以下公式求得具体指标的值，其中total表示待预测数据的总数。

  \indent     准确率(acc)：$acc = \frac{{TP + TN}}{{{\rm{total}}}}$

  \indent     召回率(rec)：$rec = \frac{{TP}}{{TP + FN}}$

  \indent     F1       值：$F1 = \frac{{2 * acc * rec}}{{acc + rec}}$}


\section{实验环境}
{\songti\xiaosihao 为了验证本文工作的有效性，本文使用Python借助TensorFlow\footnote{https://www.tensorflow.org/}深度学习框架来实现本文提出的方法。采用OWLAPI\footnote{http://owlapi.sourceforge.net/}（用于管理OWL本体的工具）本体解析工具来获得本体信息。实验是在具有64GB内存和TiTAN XP GPU 的Intel Xeon E5-2630 V4 CPU的个人工作站上进行的。本文的方法，数据集和结果可一起由该链接\footnote{https://github.com/chunyedxx/MultiOM} 下载。}

\section{实验结果}

{\songti\xiaosihao 针对现有方法的两个问题，本文设计如下实验，逐步验证本文建模思想是否正确，是否可以有效解决现有方法的两个问题，具体实验步骤由三部分构成，分别是采用本体结构信息的MOM方法与基于字符串方法的对比实验，MOM多视角结合与单个预训练模型的结果对比，MOMWH与MOM结果对比以及MOMWH$^+$与MOMWH,MOM及其他方法结果对比。

\indent 本体概念中单词向量表示主要来自文献\cite{}的链接\footnote{https://doi.org/10.5281/zenodo.1173936}，其维数设置为200。 对于一些没有向量表示的单词，我们将其随机初始化，并满足${\left|{\left|{{t_{1i}}}\right|}\right|_2}{\rm{ }} \le {\rm{ }}1$ 和${\left|{\left|{{t_{2i}}}\right|}\right|_2}{\rm{ }} \le {\rm{ }}1$。}

\subsection{采用本体结构信息的MOM方法与基于字符串方法的对比实验}
{\songti\xiaosihao 本实验采用MA-NCI-FMA和FMA-NCI-small-SNOMED两个数据集，将本文的MOM，MOM$^-$方法与StringEquiv，StringEquiv-N，StringEquiv-S，StringEquiv-SR，StringEquiv-NS，StringEquiv-NSR进行对比，以上方法中唯一的变量因素为有没有使用本体的结构信息，通过该实验来验证本体的结构信息是否有助于提升本体匹配的效果。其中，MOM和MOM$^-$方法训练时共享同组参数，具体参数如表\ref{tab:canhsu}所示：

\begin{table}[!h]
\label{tab:canhsu}
\caption{MOM模型训练时的参数配置}
\centering
\begin{tabular}{cc}
\toprule  %添加表格头部粗线
\midrule  %添加表格中横线
参数     & 参数值 \\
\hline
$d$      & $\{{\rm{50}},{\rm{100}}\}$ \\
$d_M$    & $\{ {\rm{50}} \times {\rm{50}},{\rm{100}} \times {\rm{100}}\}$ \\
$nbatch$ & $\{ {\rm{5}},{\rm{10}},20,50\}$\\
$r$      & $\{ {\rm{0}}.{\rm{01}},{\rm{0}}.{\rm{02}},{\rm{0}}.{\rm{001}}\}$\\
$nrate$  & $\{ {\rm{1}},{\rm{3}},{\rm{5}},{\rm{10}}\}$\\
$epoch$  & $1000$\\
$a$      & $\{ {\rm{0}}.{\rm{01}},{\rm{0}}.{\rm{05}},{\rm{0}}.{\rm{10}}\}$\\
$\{ {\delta _1},{\delta _2},{\delta _3},{\delta _4}\}$ & $\{ 0.8,0.95,0.65,0.3\}$\\
\bottomrule %添加表格底部粗线
\hline
\end{tabular}
\end{table}

\indent 实验结果如表\ref{tab:result1}所示，表中列出了MOM以及多种字符串方法的结果对比表。通过对比可以发现，MOM的匹配效果要比基于字符串的基准匹配系统StringEquiv-NSR的效果无论是准确率，召回率还是F值都要高。两者的差别在于MOM采用了本体的结构信息，而基于字符串的匹配系统未采用结构信息。同样，通过MOM与MOM$^-$的对比也可以发现，MOM较MOM$^-$加入了基于结构的负采样，表中MOM的匹配效果也优于MOM$^-$，所以，以上结果表明，充分考虑本体的结构信息有助于提升本体匹配的效果。


\begin{table}[!h]
	\centering
	\footnotesize
	\caption{MultiOM与基于字符串的基准匹配系统的结果对比表}
	\label{tab:result1}
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
		\multirow{Methods} &\multicolumn{5}{c|}{MA-NCI} & \multicolumn{5}{c|}{FMA-NCI-small} \\
		&Number & Correct& P &R & F1&Number & Correct& P &R & F1\\\hline
		StringEquiv & 935& 932& 0.997  & 0.615 &   0.761
		& 1501 & 1389 & 0.925  &0.517 &0.663   \\\hline
		StringEquiv-N &992 & 989&  0.997& 0.625 & 0.789
		& 1716 & 1598 & 0.931  &0.595 &  0.726 \\\hline
		StringEquiv-S &1100 & 1057& 0.961 & 0.697&  0.808
		& 2343 & 2082 & 0.889  &0.775 &  0.828 \\\hline
		StringEquiv-SR & 1162& 1094 & 0.941 & 0.722 &  0.817
		& 2343 & 2082 & 0.889  &0.775 &  0.828 \\\hline
		StringEquiv-NS & 1153& 1109& 0.962 & 0.732&   0.831
		& 2464 & 2200 & 0.893  &0.819 &  0.854  \\\hline
		StringEquiv-NSR &1211 & 1143  & 0.943  & 0.753 &  0.838
		& 2467 & 2203 & 0.893  &0.820 &  0.855  \\\hline
		MOM$^-$&1484 & 1296   & 0.873  & 0.855 &  0.864
		&  2471   &  2192  &  0.887  & 0.809   & 0.846    \\\hline
		MOM&1445 & 1287   & 0.891  & 0.849 &  0.869
		&  2470  & 2195  &    0.889 &   0.817 &   0.851  \\\hline
	\end{tabular}
\end{table}}
}
\subsection{MOM多视角结合与单个预训练模型的结果对比}
{\songti\xiaosihao 本实验针对MA-NCI-FMA数据集，将本文的MOM，MOM$^-$，MOM-S，MOM-L，MOM-R，MOM-RS方法进行对比，以上方法中唯一的变量因素为不同的方法对应的视角不同，通过该实验来验证不同的视角是否可以学习到不同方面的特征，多视角的融合是否有助于提升本体匹配的效果。其中，MOM和MOM$^-$ 方法训练时共享同组参数，其他模型的参数也如表\ref{tab:canhsu}所示。实验中也对比了对于同一视角的模型在不同的阈值下模型的匹配效果。

\indent 实验结果如表\ref{tab:result2,tab:result3}所示：
\begin{table}[!h]
	\centering
	\caption{MultiOM分别将不同视角的模型组合后的效果对比图}
	\footnotesize
	\label{tab:result2}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Methods &Number & Correct& P &R & F1\\\hline
		TFIDF (threshold$=0.8$) &  985        &    976    &    0.991         &   0.644        & 0.780     \\\hline
		MOM-$L$ (threshold$=0.8$)     & 1286& 1175& 0.914  & 0.775 &   0.839  \\\hline
		MOM-$S^-$ (threshold$=0.95$)  & 1836& 1109      &    0.604   & 0.732  & 0.662    \\\hline
		MOM-$S$ (threshold$=0.95$)    & 1189& 1097& 0.923  & 0.724 &   0.811  \\\hline
		MOM-$R$ (Random initialization, threshold$=0.95$) &    709   &   680       &  0.959   &  0.449   &  0.612      \\\hline
		MOM-$R$ (loss function \ref{Pretaining},  threshold$=0.95$)     & 833& 789   & 0.948    & 0.520 &   0.672  \\\hline
		MOM-$RS^-$ (threshold$=0.95$)   & 1271& 1147& 0.902  & 0.757 &   0.823  \\\hline
		MOM-$RS$ (threshold$=0.95$)   & 1237& 1138& 0.920  & 0.751 &   0.827  \\\hline
        MOM-$S^-$ (threshold$=0.95$)  & 1821& 1110      &    0.610   & 0.733  & 0.666    \\\hline
        MOM$^-$&1484 & 1296   & 0.873  & 0.855 &  0.864 \\\hline
        MOM&1445 & 1287   & 0.891  & 0.849 &  0.869  \\\hline
	\end{tabular}
\end{table}

\begin{table}[!h]
	\centering
	\caption{MOM不同视角的模型效果对比}
	\label{tab:result3}
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		              &MOM-$L$ & MOM-$S$& MOM-$R$ &MOM-$LS$ & MOM-$LR$ & MOM-$SR$ \\\hline
		MOM-$L$ &  --  &  176  &   463  & --    &  --     &    154    \\\hline
		MOM-$S$ &  99  &   -- &   354  & --   &   45    &  --      \\\hline
		MOM-$R$  & 78   & 46   &   -- &   24  &   --    &    --    \\\hline
	\end{tabular}
\end{table}

\indent 表\ref{tab:result2}展示了MOM分别将不同视角的模型组合后的效果对比。对比数据可以发现，将任意的两个不同视角模型的匹配结果组合后，匹配效果会比单个视角的模型匹配效果要好，同时三个视角融合后的匹配效果即MOM无论是正确率，召回率还是F值都优于其他任意组合或模型的效果，这也说明不同视角的模型所学特征是存在差别的，不同方面的特征融合后，最总才有MOM的整体效果的提升。为进一步确定以上结论，本文通过表\ref{tab:result3}中的结果进一步说明。表中的结果指将三个视角的模型匹配结果互相比较，并与他们的组合进行比较，可以发现，不同视角的模型总能找到不同于其他视角的匹配，这也进一步证明了不同视角的模型所学特征存在互补性，多视角的建模方式在本体匹配任务中是有效的。}

\subsection{MOMWH与MOM结果对比}
{\songti\xiaosihao 本实验针对MA-NCI-FMA和FMA-NCI-small-SNOMED两个数据集，将本文的MOMWH，MOMWH$^-$，MOM，MOM$^-$，以及MOMWH和MOM中多个不同视角的模型，将MOMWH与MOM中相对应的模型进行对比，以上方法中唯一的变量因素为MOM中的模型参数是人工调节的参数，MOMWH中模型参数一部分为采用HORD 算法优化得到的参数，通过该实验来验证HORD算法是否可以改进表示学习模型学得的概念向量表示，是否可以因此提升本体匹配的效果。其中，MOM方法训练与其匹配算法的参数如图，MOMWH模型训练时的参数，实验中也对比了对于同一视角的模型在不同的阈值下模型的匹配效果。

\indent 该实验首先需采用HORD算法对MOM中的表示学习模型MOM-S，MOM-R进行超参数优化。HORD算法的执行与MOM-L或MOM-R训练的方式有所不同，需给出模型（如MOM-S，MOM-R类模型）中待优化超参数的区间范围，而不用给定具体的值。对于模型中没有进行进一步优化的参数，则采用MOM中对应模型相同的参数。

\indent MOM-S和MOM-R的超参数包括训练轮数（epoch），学习率（Learning rate），向量维数（Dimensions），以及批次数（nbatchs），以及负采样率(Negative samping rate)。为降低参数调节的计算量，提升MOMWH模型效率，本文未对以上六种参数都进行优化。首先，MOM中的表示学习模型的参数规模由向量维数大致确定，而参数的规模应与本体数据集中的概念以及关系的多少有关，在二者数量不变的前提下，可大致确定向量的维数（Dimensions），保证可以适用于匹配任务即可。所以，MOM中未对这两个参数进行调节；其次，训练轮数（Epoch）根据人工经验得到的值为1000，该参数决定了模型学习时间的长短，为保证参数优化后，可以提升MOM的的效率，我们将其固定为500，通过调节剩余的三个参数：学习率（Learning rate），批次数（nbatchs）和负采样率(Negative samping rate)。所以MOMWH模型的参数设置如表\ref{tab:canhsu2}所示。
\begin{table}[!h]
\label{tab:canhsu2}
\caption{MOMWH模型中HORD超参调节的参数设置}
\centering
\begin{tabular}{cc}
\toprule  %添加表格头部粗线
\midrule  %添加表格中横线
参数     & 参数值 \\
\hline
$d$      & $\{{\rm{50}},{\rm{100}}\}$ \\
$d_M$    & $\{ {\rm{50}} \times {\rm{50}},{\rm{100}} \times {\rm{100}}\}$ \\
$nbatch$ & $\left[ {10,60} \right]$\\
$r$      & $\left[ {0.001,0.05} \right]$\\
$nrate$  & $\left[ {3,15} \right]$\\
$epoch$  & $500$\\
$n_0$    & $6$ \\
$N_{max}$& $25$\\
\bottomrule %添加表格底部粗线
\end{tabular}
\end{table}

\indent MOMWH中通过HORD算法进行超参优化后，MOM-S与MOM-R的最优超参配置对比如表\ref{tab:canhsu3}所示：
\begin{table}[!h]
\label{tab:canhsu3}
\caption{HORD算法进行超参数优化前后的参数对比}
\centering
\begin{tabular}{ccc|cc}
\toprule  %添加表格头部粗线
\midrule  %添加表格中横线
参数名   & \multicolumn{2}{c|}{MOM-S}&\multicolumn{2}{c}{ MOM-R}\\        
         & 优化前   & 优化后   & 优化前  & 优化后 \\
\hline
$nbatch$ &   5      &   8      &  10     &   12 \\
$r$      &   0.0100 &   0.0131 &  0.0100 &   0.0107\\ 
$nrate$  &   3      &   6      &  5      &   8\\
$epoch$  &   1000   &   500    &  1000   &   500\\
\bottomrule %添加表格底部粗线
\end{tabular}
\end{table}

\indent 除了以上经过HORD算法优化过的参数外，其他参数依然与原模型中参数相同。MOMWH与MOM及各种基准匹配结果对比如表\ref{tab:result5，tab:result6}所示。

\begin{table}[!h]
	\centering
	\footnotesize
	\caption{MOMWH与MOM以及各种基准匹配结果的对比}
	\label{tab:result5}
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
		\multirow{Methods} &\multicolumn{5}{c|}{MA-NCI} & \multicolumn{5}{c|}{FMA-NCI-small} \\
		&Number & Correct& P &R & F1&Number & Correct& P &R & F1\\\hline
		StringEquiv & 935& 932& 0.997  & 0.615 &   0.761
		& 1501 & 1389 & 0.925  &0.517 &0.663   \\\hline
		StringEquiv-N &992 & 989&  0.997& 0.625 & 0.789
		& 1716 & 1598 & 0.931  &0.595 &  0.726 \\\hline
		StringEquiv-S &1100 & 1057& 0.961 & 0.697&  0.808
		& 2343 & 2082 & 0.889  &0.775 &  0.828 \\\hline
		StringEquiv-SR & 1162& 1094 & 0.941 & 0.722 &  0.817
		& 2343 & 2082 & 0.889  &0.775 &  0.828 \\\hline
		StringEquiv-NS & 1153& 1109& 0.962 & 0.732&   0.831
		& 2464 & 2200 & 0.893  &0.819 &  0.854  \\\hline
		StringEquiv-NSR &1211 & 1143  & 0.943  & 0.753 &  0.838
		& 2467 & 2203 & 0.893  &0.820 &  0.855  \\\hline
		MOM$^-$&1484 & 1296   & 0.873  & 0.855 &  0.864
		&  2471   &  2192  &  0.887  & 0.809   & 0.846    \\\hline
		MOM&1445 & 1287   & 0.891  & 0.849 &  0.869
		&  2470  & 2195  &    0.889 &   0.817 &   0.0.851  \\\hline
        MOMWH$^-$&1462 & 1296   & 0.886  & 0.855 &  0.870
		&  2469   &  2192  &  0.888  & 0.809   & 0.847    \\\hline
        MOMWH&1431 & 1290   & 0.901  & 0.851 &  0.0.876
		&  2466  & 2200  &    0.892 &   0.819 &   0.854  \\\hline
	\end{tabular}
\end{table}

\begin{table}[!h]
	\centering
	\caption{MOMWH与MOM不同视角模型及其组合后的效果对比}
	\footnotesize
	\label{tab:result6}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Methods &Number & Correct& P &R & F1\\\hline
		MOM-$S^-$ (threshold$=0.95$)  & 1836& 1109      &    0.604   & 0.732  & 0.662    \\\hline
		MOM-$S$ (threshold$=0.95$)    & 1189& 1097& 0.923  & 0.724 &   0.811  \\\hline
		MOM-$R$ (Random initialization, threshold$=0.95$) &    709   &   680       &  0.959   &  0.449   &  0.612      \\\hline
		MOM-$R$ (loss function \ref{Pretaining},  threshold$=0.95$)     & 833& 789   & 0.948    & 0.520 &   0.672  \\\hline
		MOM-$RS^-$ (threshold$=0.95$)   & 1271& 1147& 0.902  & 0.757 &   0.823  \\\hline
		MOM-$RS$ (threshold$=0.95$)   & 1237& 1138& 0.920  & 0.751 &   0.827  \\\hline
        MOM$^-$&1484 & 1296   & 0.873  & 0.855 &  0.864 \\\hline
        MOM&1445 & 1287   & 0.891  & 0.849 &  0.869  \\\hline
        MOMWH-$S^-$ (threshold$=0.95$)  & 1821& 1110      &    0.610   & 0.733  & 0.666    \\\hline
		MOMWH-$S$ (threshold$=0.95$)    & 1170& 1099& 0.939  & 0.725 &   0.818  \\\hline
		MOMWH-$R$ (Random initialization, threshold$=0.95$) &    705   &   680       &  0.965   &  0.449   &  0.613      \\\hline
		MOMWH-$R$ (loss function \ref{Pretaining},  threshold$=0.95$)     & 824& 794   & 0.964    & 0.524 &   0.679  \\\hline
		MOMWH-$RS^-$ (threshold$=0.95$)   & 1263& 1147& 0.908  & 0.757 &   0.826  \\\hline
		MOMWH-$RS$ (threshold$=0.95$)   & 1221& 1140& 0.934  & 0.752 &   0.833  \\\hline
		MOMWH$^-$&1462 & 1296   & 0.886  & 0.855 &  0.870 \\\hline
		MOMWH&1431 & 1290   & 0.901  & 0.851 &  0.876  \\\hline
	\end{tabular}
\end{table}

\indent 如表\ref{tab:canhsu3}所示，采用HORD算法进行超参优化后与优化前的模型参数对比，对比可以发现，超参数优化后，对于同样的模型之前训练需要1000个epoch，而优化后只需要500个epoch，较之前只需要一半的训练次数，极大的提升了模型的效率。

\indent 对比表\ref{tab:result5}中的数据，MOMWH无论是准确率，召回率还是F值都较之前的MOM都有所提升，MOMWH- 的性能也优于MOM-，并且MOMWH的结果可以与StringEquiv-NSR相当。MOMWH是基于MOM采用HORD算法进行超参优化的模型，显然，经过优化后的参数，可以学到更加准确的特征；另外，无论是MOM与MOM-还是MOMWH与MOMWH-都说明同利用了改进后的结构负采样，效果依然会有所提高，也进一步验证了本体的结构信息有助于本体匹配任务效果的提升。

\indent 表\ref{tab:result6}展示了MOMWH与MOM分别将不同视角的模型组合后的效果对比。通过各个视角模型的匹配结果详细对比可以发现，采用HORD算法进行参数优化后，基本上对各个视角的匹配结果都有所提升。众多的结果说明，经过参数优化后，不仅可以使得各个模型错误的匹配得以减少同时可以寻找到一些正确匹配，以上结果进一步说明参数优化后可以帮助表示学习模型学得更加合适得特征，获得得概念向量表示更加有效。}


\subsection{MOMWH$^+$与MOMWH,MOM及其他方法结果对比}
{\songti\xiaosihao 本实验针对MA-NCI-FMA和FMA-NCI-small-SNOMED两个数据集，将本文的MOMWH$^+$与MOMWH,MOM与OAEI2018的顶级系统其他各类匹配方法LogMapBio，XMap等以及SCBOW+DAE方法的匹配效果进行比较。根据之前的实验，发现利用改进后的TF-IDF算法的效果准确率有$0.991$,其召回率不高，主要是因为我们利用的是预训练词向量，只包含其字面语义信息，并未对这些预训练词向量基于当前语境进行改进，所以而未包含该本体匹配任务下的语义信息。考虑到以上原因，本文提出进一步在MOM框架下，基于BERT模型对其进行改进，并采用HORD算法进行超参数优化的MOMWH$^+$模型，具体的对比结果如表\ref{tab:result8} 所示。


\begin{table}[!h]
	\centering
	\caption{本文模型与OAEI2018顶级系统以及基于表示学习的SCBOW+DAE模型的效果比较}
	\label{tab:result8}
	\footnotesize
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
		\multirow{Methods} &\multicolumn{5}{c|}{MA-NCI} & \multicolumn{5}{c|}{FMA-NCI-small} \\
		  &Number & Correct& P &R & F1  &Number & Correct& P &R & F1\\\hline
		SCBOW + DAE 	& 1399 & 1356   & 0.969  & 0.906 &   0.938
		& 2282 & 2227  & 0.976 & 0.889    & 0.930 \\\hline
        \textbf{BertMultiOM+HORD}& \textbf{1436} & \textbf{1363}  & \textbf{0.949}  & \textbf{0.891} &  \textbf{0.919}
		&  \textbf{2787}  & \textbf{2647}  &    \textbf{0.950} &   \textbf{0.907} &   \textbf{0.928} \\\hline
		\textbf{BertMultiOM}& \textbf{1441} & \textbf{1363}  & \textbf{0.946}  & \textbf{0.891} &  \textbf{0.918}
		&  \textbf{2793}  & \textbf{2647}  &    \textbf{0.948} &   \textbf{0.907} &   \textbf{0.927} \\\hline
        LogMapBio   &1550 & 1376  &  0.888& 0.908 & 0.898
		& 2776  & 2632  & 0.948  & 0.902   & 0.921 \\\hline
		POMAP++     &1446 &  1329 &  0.919& 0.877 & 0.897
		&2414  &2363  & 0.979  & 0.814    & 0.889 \\\hline
		XMap             &1413 &   1312& 0.929 & 0.865&  0.896
		&2315  & 2262 & 0.977  & 0.783   & 0.869 \\\hline
		LogMap        & 1387&   1273  & 0.918 & 0.846&   0.880
        & 2747 & 2593  & 0.944 &  0.897  & 0.920\\\hline
		SANOM         &1450 & 1287   & 0.888 & 0.844&  0.865
		& --  & --  & -- & --   & -- \\\hline
		FCAMapX      & 1274&  1199   & 0.941 & 0.791 &  0.859
		& 2828 &  2681  & 0.948 & 0.911   & 0.929 \\\hline
		\textbf{MultiOM}& \textbf{1445} & \textbf{1287}  & \textbf{0.891}  & \textbf{0.849} &  \textbf{0.869}
		&  \textbf{2330}  & \textbf{2195}  &    \textbf{0.942} &   \textbf{0.817} &   \textbf{0.875} \\\hline
	\end{tabular}
\end{table}

\indent 通过比较表\ref{tab:result8}中的数据可以发现，MOMWH$^+$的效果较MOMWH和MOM有了较大幅度的提升，并且从召回率与F值来综合比较，MOMWH$^+$的效果比LogMapBio，POMAP++等方法都要有优势。另外与第一名的SCBOW+DAE相比，在FMA-NCI-small-SNOMED数据集中的效果，也与之可以相提并论。

\indent 综上实验，可以进一步验证说明本文提出的方法，可以在一定程度上解决现有基于深度学习的方法的问题，本文充分利用本体中的结构信息，并且对本文提出的基于表示学习的方法MOM等模型进行超参数优化，以四个不同的实验，进一步验证了充分利用本体的结构信息有助于提升本体匹配的效果，采用多视角的建模方式是实现本体匹配任务的有效方式之一，以及采用HORD算法对表示学习模型的超参数进行优化，可以改进其获得概念向量表示，从而提升本体匹配的效果，本文模型MOMWH$^+$，MOMWH也取得了比较好的效果。
}



%\indent -OM-L：MultiOM+HORD未对基于上下文视角的预训练模型进行超参优化，所以依然沿用OM-L表示改进后的TF-IDF算法的效果；

%\indent -OMH-R：MultiOM中采用OM-R表示基于外部资源视角的预训练模型的匹配结果，MultiOM+HORD采用HORD算法对其参数进行优化，故这里简记为OMH-R；

%\indent -OMH-S：MultiOM中采用OM-S表示基于结构视角的预训练模型的匹配结果，MultiOM+HORD采用HORD算法对其参数进行优化，故这里简记为OMH-S；

%\indent -OMH-RS：是OMH-R与OMH-S采用多视角匹配算法进行组合后的结果；

%\indent -OMH-SR$^-$：未采用本文改进的结构负采样，将OMH-R与OMH-S 采用多视角匹配算法进行组合后的结果；

%\indent -MultiOMH：以同样的方式，这里采用MultiOMH表示HORD算法改进MultiOM后的整体结果；MultiOMH$^-$表示MultiOM未采用本文改进的结构负采样的结果。

\ifx\allfiles\undefined
\end{document}
\fi
