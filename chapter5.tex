\ifx\allfiles\undefined
\documentclass[a4paper,12.15pt,CJK,oneside]{article}
\begin{document}
%\pagestyle{plain}
\else
\fi

\chapter{实验与评估}
\thispagestyle{fancy} \fancyhead[L]{\songti \wuhao
重庆师范大学硕士学位论文}\fancyhead[R]{\songti \wuhao 4~实验与评估}

\section{评估指标与数据集}

\subsection{数据集介绍}
 {\songti\xiaosihao 本节简要概述在本体匹配实验中使用的四种本体，其中两个本体FMA(Foundational Model of Anatomy)和MA(Adult Mouse
anatomical)分别表示解剖学本体和成年小鼠解剖学本体，二者都是纯粹的解剖学本体论，而另外两个SNOMED CT和NCI是涉及更广的生物医学本体，解剖学只是它们描述的一个子领域\cite{}。虽然这些本体的最新版本是可用的，但是本文在整个本体匹配任务中参考了出现在OAEI(Ontology Alignment Evaluation Initiative) 中的版本，以便在本体匹配系统之间进行比较。

\indent FMA(Foundational Model of Anatomy)：这是一个一直被更新的本体，自1994年由华盛顿大学开发并维护\cite{}，其目的是以机器可读的形式概念化人体的表型结构。

\indent MA(Adult Mouse Anatomical Dictionary)：该本体是描述成年小鼠解剖结构的结构的词汇表\cite{}。

\indent NCI(NCI Thesaurus)：该本体提供了癌症的标准词汇\cite{}，其解剖学子域描述人类自有的生物结构、液体和物质。

\indent SNOMED(SNOMED Clinical Terms)：该本体是一个系统化，组织化的机器可读的医学术语集合，提供临床文档和报告中使用的代码、术语、同义词和定义\cite{}。

\indent 以上本体的概念及对应匹配数量具体如图\ref{tab:dataset}：
\begin{table}[h]
\centering
\caption{本体概念数及对应匹配数量}
\label{tab:dataset}
\begin{tabular}{ccccc}
\hline
 源本体&概念数&目标本体&概念数&匹配数\\
\hline
    MA&2744&NCI&3304&1489\\
\hline
 FMA&3696&NCI&6488&2504\\
\hline
  FMA&10157&SNOMED&13412&7774\\
\hline
\end{tabular}
\end{table}

\subsection{评估指标介绍}
  {\songti\xiaosihao
  OAEI为本体匹配提供了统一的评估标准，评估的指标主要是：匹配的准确率，召回率和F1值。为阐述以上三个指标含义，首先介绍以下概念：

  \indent      TP(True Positives) ：表示模型将正例预测为正例的数据条数；

  \indent      FP(False Positives)：表示模型将负例预测为正例的数据条数；

  \indent      FN(False Negatives)：表示模型将正例预测为负例的数据条数；

  \indent      TN(True Negatives) ：表示模型将负例预测为负例的数据条数；

\indent 所以，由以上四个参数值，可根据以下公式求得具体指标的值，其中total表示待预测数据的总数。

  \indent     准确率(acc)：$acc = \frac{{TP + TN}}{{{\rm{total}}}}$

  \indent     召回率(rec)：$rec = \frac{{TP}}{{TP + FN}}$

  \indent     F1       值：$F1 = \frac{{2 * acc * rec}}{{acc + rec}}$}


\section{实验环境}
{\songti\xiaosihao 为了验证MultiOM的有效性，本文使用Python借助TensorFlow\footnote{https://www.tensorflow.org/}（一种非常流行的深度学习框架）来评估MultiOM模型的效果。采用OWLAPI\footnote{http://owlapi.sourceforge.net/}（用于管理OWL本体的工具）本体解析工具来获得本体信息。实验是在具有64GB内存和TiTAN XP GPU的Intel Xeon E5-2630 V4 CPU的个人工作站上进行的。本文的方法，数据集和结果可一起由该链接\footnote{https://github.com/chunyedxx/MultiOM}下载。}

\section{评估结果}

\subsection{实验设计}
{\songti\xiaosihao 针对现有方法的两个问题，本文设计如下实验，逐步验证本文建模思想是否正确，是否可以有效解决现有方法的两个问题，具体实验步骤如下：
\indent 1. 使用结构信息的MOM与字符串方法的结果对比。MOM模型建模过程中使用了第三方本体的结构信息，以及两个待匹配本体的结构信息，为验证结构信息是否有助于提升本体匹配效果，设计该实验，在两个数据集中，将MOM与未采用结构信息的各类字符串方法作对比，验证MOM的有效性。
\indent 2. MOM多视角结合与单个训练连模型的结果对比。MOM由是基于多视角思想构建的表示学习模型，为验证多视角的建模思想是否适用于本体匹配任务，本文设计该实验，一方面将MOM中不同视角的模型进行组合，与单个视角的预训练模型作对比，另一方面对不同视角的预训练模型匹配效果进行对比，验证不同视角的模型是否可以学得不同的特征，最终来验证多视角的思想是否可行。
\indent 3. MOWHM与MOM结果对比。MOWHM是采用HORD算法对MOM算法进行超参优化的模型，为证明HORD算法对MOM中的MOM-S模型和MOM-R模型进行超参优化后，是否有助于提升MOM的本体匹配性能，设计该实验，来验证
\indent 4. MOWHM$^+$与MOMWH,MOM及其他方法结果对比

\subsection{使用结构信息的MOM与字符串方法的结果对比}

\begin{table}[!h]
	\centering
	\footnotesize
	\caption{MultiOM以及各种基准匹配结果的对比图}
	\label{tab:result1}
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
		\multirow{Methods} &\multicolumn{5}{c|}{MA-NCI} & \multicolumn{5}{c|}{FMA-NCI-small} \\
		&Number & Correct& P &R & F1&Number & Correct& P &R & F1\\\hline
		StringEquiv & 935& 932& 0.997  & 0.615 &   0.761
		& 1501 & 1389 & 0.925  &0.517 &0.663   \\\hline
		StringEquiv-N &992 & 989&  0.997& 0.625 & 0.789
		& 1716 & 1598 & 0.931  &0.595 &  0.726 \\\hline
		StringEquiv-S &1100 & 1057& 0.961 & 0.697&  0.808
		& 2343 & 2082 & 0.889  &0.775 &  0.828 \\\hline
		StringEquiv-SR & 1162& 1094 & 0.941 & 0.722 &  0.817
		& 2343 & 2082 & 0.889  &0.775 &  0.828 \\\hline
		StringEquiv-NS & 1153& 1109& 0.962 & 0.732&   0.831
		& 2464 & 2200 & 0.893  &0.819 &  0.854  \\\hline
		StringEquiv-NSR &1211 & 1143  & 0.943  & 0.753 &  0.838
		& 2467 & 2203 & 0.893  &0.820 &  0.855  \\\hline
		MultiOM$^-$&1484 & 1296   & 0.873  & 0.855 &  0.864
		&  2471   &  2192  &  0.887  & 0.809   & 0.846    \\\hline
		MultiOM&1445 & 1287   & 0.891  & 0.849 &  0.869
		&  2470  & 2195  &    0.889 &   0.817 &   0.0.851  \\\hline
	\end{tabular}
\end{table}}

\subsection{MOM多视角结合与单个预训练模型的结果对比}


\begin{table}[!h]
	\centering
	\caption{MultiOM分别将不同视角的模型组合后的效果对比图}
	\footnotesize
	\label{tab:result2}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Methods &Number & Correct& P &R & F1\\\hline
		TFIDF (threshold$=0.8$) &  985        &    976    &    0.991         &   0.644        & 0.780     \\\hline
		OM-$L$ (threshold$=0.8$)     & 1286& 1175& 0.914  & 0.775 &   0.839  \\\hline
		OM-$S^-$ (threshold$=0.95$)  & 1836& 1109      &    0.604   & 0.732  & 0.662    \\\hline
		OM-$S$ (threshold$=0.95$)    & 1189& 1097& 0.923  & 0.724 &   0.811  \\\hline
		OM-$R$ (Random initialization, threshold$=0.95$) &    709   &   680       &  0.959   &  0.449   &  0.612      \\\hline
		OM-$R$ (loss function \ref{Pretaining},  threshold$=0.95$)     & 833& 789   & 0.948    & 0.520 &   0.672  \\\hline
		OM-$RS^-$ (threshold$=0.95$)   & 1271& 1147& 0.902  & 0.757 &   0.823  \\\hline
		OM-$RS$ (threshold$=0.95$)   & 1237& 1138& 0.920  & 0.751 &   0.827  \\\hline
        OM-$S^-$ (threshold$=0.95$)  & 1821& 1110      &    0.610   & 0.733  & 0.666    \\\hline
        MultiOM$^-$&1484 & 1296   & 0.873  & 0.855 &  0.864 \\\hline
        MultiOM&1445 & 1287   & 0.891  & 0.849 &  0.869  \\\hline
	\end{tabular}
\end{table}


\begin{table}[!h]
	\centering
	\caption{MultiOM不同视角的模型效果对比}
	\label{tab:result3}
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		              &OM-$L$ & OM-$S$& OM-$R$ &OM-$LS$ & OM-$LR$ & OM-$SR$ \\\hline
		OM-$L$ &  --  &  176  &   463  & --    &  --     &    154    \\\hline
		OM-$S$ &  99  &   -- &   354  & --   &   45    &  --      \\\hline
		OM-$R$  & 78   & 46   &   -- &   24  &   --    &    --    \\\hline
	\end{tabular}
\end{table}

\subsection{MOMWH与MOM结果对比}

\begin{table}[!h]
	\centering
	\footnotesize
	\caption{MultiOMH与MultiOM以及各种基准匹配结果的对比图}
	\label{tab:result5}
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
		\multirow{Methods} &\multicolumn{5}{c|}{MA-NCI} & \multicolumn{5}{c|}{FMA-NCI-small} \\
		&Number & Correct& P &R & F1&Number & Correct& P &R & F1\\\hline
		StringEquiv & 935& 932& 0.997  & 0.615 &   0.761
		& 1501 & 1389 & 0.925  &0.517 &0.663   \\\hline
		StringEquiv-N &992 & 989&  0.997& 0.625 & 0.789
		& 1716 & 1598 & 0.931  &0.595 &  0.726 \\\hline
		StringEquiv-S &1100 & 1057& 0.961 & 0.697&  0.808
		& 2343 & 2082 & 0.889  &0.775 &  0.828 \\\hline
		StringEquiv-SR & 1162& 1094 & 0.941 & 0.722 &  0.817
		& 2343 & 2082 & 0.889  &0.775 &  0.828 \\\hline
		StringEquiv-NS & 1153& 1109& 0.962 & 0.732&   0.831
		& 2464 & 2200 & 0.893  &0.819 &  0.854  \\\hline
		StringEquiv-NSR &1211 & 1143  & 0.943  & 0.753 &  0.838
		& 2467 & 2203 & 0.893  &0.820 &  0.855  \\\hline
		MultiOM$^-$&1484 & 1296   & 0.873  & 0.855 &  0.864
		&  2471   &  2192  &  0.887  & 0.809   & 0.846    \\\hline
		MultiOM&1445 & 1287   & 0.891  & 0.849 &  0.869
		&  2470  & 2195  &    0.889 &   0.817 &   0.0.851  \\\hline
        MultiOMH$^-$&1462 & 1296   & 0.886  & 0.855 &  0.870
		&  2469   &  2192  &  0.888  & 0.809   & 0.847    \\\hline
        MultiOMH&1431 & 1290   & 0.901  & 0.851 &  0.0.876
		&  2466  & 2200  &    0.892 &   0.819 &   0.854  \\\hline
	\end{tabular}
\end{table}}

\begin{table}[!h]
	\centering
	\caption{MultiOMH与MultiOM分别将不同视角的模型组合后的效果对比图}
	\footnotesize
	\label{tab:result6}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Methods &Number & Correct& P &R & F1\\\hline
		OM-$S^-$ (threshold$=0.95$)  & 1836& 1109      &    0.604   & 0.732  & 0.662    \\\hline
		OM-$S$ (threshold$=0.95$)    & 1189& 1097& 0.923  & 0.724 &   0.811  \\\hline
		OM-$R$ (Random initialization, threshold$=0.95$) &    709   &   680       &  0.959   &  0.449   &  0.612      \\\hline
		OM-$R$ (loss function \ref{Pretaining},  threshold$=0.95$)     & 833& 789   & 0.948    & 0.520 &   0.672  \\\hline
		OM-$RS^-$ (threshold$=0.95$)   & 1271& 1147& 0.902  & 0.757 &   0.823  \\\hline
		OM-$RS$ (threshold$=0.95$)   & 1237& 1138& 0.920  & 0.751 &   0.827  \\\hline
        MultiOM$^-$&1484 & 1296   & 0.873  & 0.855 &  0.864 \\\hline
        MultiOM&1445 & 1287   & 0.891  & 0.849 &  0.869  \\\hline
        OMH-$S^-$ (threshold$=0.95$)  & 1821& 1110      &    0.610   & 0.733  & 0.666    \\\hline
		OMH-$S$ (threshold$=0.95$)    & 1170& 1099& 0.939  & 0.725 &   0.818  \\\hline
		OMH-$R$ (Random initialization, threshold$=0.95$) &    705   &   680       &  0.965   &  0.449   &  0.613      \\\hline
		OMH-$R$ (loss function \ref{Pretaining},  threshold$=0.95$)     & 824& 794   & 0.964    & 0.524 &   0.679  \\\hline
		OMH-$RS^-$ (threshold$=0.95$)   & 1263& 1147& 0.908  & 0.757 &   0.826  \\\hline
		OMH-$RS$ (threshold$=0.95$)   & 1221& 1140& 0.934  & 0.752 &   0.833  \\\hline
		MultiOMH$^-$&1462 & 1296   & 0.886  & 0.855 &  0.870 \\\hline
		MultiOMH&1431 & 1290   & 0.901  & 0.851 &  0.876  \\\hline
	\end{tabular}
\end{table}

\subsection{MOMWH$^+$与MOMWH,MOM及其他方法结果对比}


\begin{table}[!h]
	\centering
	\caption{BertMultiOM+HORD与MultiOM，OAEI2018顶级系统以及基于表示学习的SCBOW+DAE模型的效果比较}
	\label{tab:result8}
	\footnotesize
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
		\multirow{Methods} &\multicolumn{5}{c|}{MA-NCI} & \multicolumn{5}{c|}{FMA-NCI-small} \\
		  &Number & Correct& P &R & F1  &Number & Correct& P &R & F1\\\hline
		SCBOW + DAE 	& 1399 & 1356   & 0.969  & 0.906 &   0.938
		& 2282 & 2227  & 0.976 & 0.889    & 0.930 \\\hline
        \textbf{BertMultiOM+HORD}& \textbf{1436} & \textbf{1363}  & \textbf{0.949}  & \textbf{0.891} &  \textbf{0.919}
		&  \textbf{2787}  & \textbf{2647}  &    \textbf{0.950} &   \textbf{0.907} &   \textbf{0.928} \\\hline
		\textbf{BertMultiOM}& \textbf{1441} & \textbf{1363}  & \textbf{0.946}  & \textbf{0.891} &  \textbf{0.918}
		&  \textbf{2793}  & \textbf{2647}  &    \textbf{0.948} &   \textbf{0.907} &   \textbf{0.927} \\\hline
        LogMapBio   &1550 & 1376  &  0.888& 0.908 & 0.898
		& 2776  & 2632  & 0.948  & 0.902   & 0.921 \\\hline
		POMAP++     &1446 &  1329 &  0.919& 0.877 & 0.897
		&2414  &2363  & 0.979  & 0.814    & 0.889 \\\hline
		XMap             &1413 &   1312& 0.929 & 0.865&  0.896
		&2315  & 2262 & 0.977  & 0.783   & 0.869 \\\hline
		LogMap        & 1387&   1273  & 0.918 & 0.846&   0.880
        & 2747 & 2593  & 0.944 &  0.897  & 0.920\\\hline
		SANOM         &1450 & 1287   & 0.888 & 0.844&  0.865
		& --  & --  & -- & --   & -- \\\hline
		FCAMapX      & 1274&  1199   & 0.941 & 0.791 &  0.859
		& 2828 &  2681  & 0.948 & 0.911   & 0.929 \\\hline
		\textbf{MultiOM}& \textbf{1445} & \textbf{1287}  & \textbf{0.891}  & \textbf{0.849} &  \textbf{0.869}
		&  \textbf{2330}  & \textbf{2195}  &    \textbf{0.942} &   \textbf{0.817} &   \textbf{0.875} \\\hline
	\end{tabular}
\end{table}


{\songti\xiaosihao 本文选择几种策略来构建基准值以验证MultiOM模型的有效性。以下是本文实验策略的构造细节，其中N代表Normalization，对概念进行标准化；S表示Synonym，利用同义词词典扩展当前概念；R表示Reference Ontology，使用第三方本体。

\indent 为突出我们提出的负采样的效果，在模块标签或合并标签上添加符号“-”表示该模块未配备针对结构关系量身定制的负采样。


\indent MultiOM训练时，使用随机梯度下降（SGD）作为优化方法，超参数的配置如下所示：概念和矩阵的维度设置为${\rm{d=\{ 50,100\}}}$和$ {{\rm{d}}_M}{\rm{ = }}\{ {\rm{50}} \times {\rm{50}},{\rm{100}} \times {\rm{100}}\}$。SGD的批次数设置为${\rm{Nbatch=\{ 5,10}},20,50\}$。学习率$r$可选参数有${\rm{\{ 0}}{\rm{.01,0}}{\rm{.02,0}}{\rm{.001\} }}$。对于每个正例，负采样率可选的有${\rm{\{ 1,3,5,10\} }}$。训练的轮数（epoch）设置为1000。在基于上下文模块中，本体概念中单词向量表示主要来自文献\cite{}的链接\footnote{https://doi.org/10.5281/zenodo.1173936}，其维数设置为200。 对于一些没有向量表示的单词，我们将其随机初始化，并满足${\left|{\left|{{t_{1i}}}\right|}\right|_2}{\rm{ }} \le {\rm{ }}1$和${\left|{\left|{{t_{2i}}}\right|}\right|_2}{\rm{ }} \le {\rm{ }}1$。在外部资源视角的模块中，我们使用TransE\cite{}，ConvE\cite{}和预训练函数7初始化第三方本体中概念的向量表示。在损失函数7中将$a$设置为${\rm{\{ 0}}{\rm{.01,0}}{\rm{.05,0}}{\rm{.10\} }}$，以控制概念向量的语义距离。对于负采样策略，我们利用本体中概念的结构信息。当针对一个概念进行负采样替换时，我们参照这个概念的结构关系信息，根据剩余概念与该概念的结构关系进行替换。负采样替换时，与该概念存在“disjointwith”关系的概念优先级最高，并且排除与概念存在“SubClassOf”关系和“PartOF”关系的概念。最后，通过多视角的组合匹配策略生成MultiOM最终匹配结果，并将相关阈值设置为${\delta _1} = 0.8$，${\delta _2} = 0.95$, ${\delta _1} = 0.65$, ${\delta _4} = 0.3$。}\\

\subsection{预测结果与分析}
{\songti\xiaosihao 表\ref{tab:result1}列出了MultiOM以及各种基准匹配结果的对比图。通过对比可以发现，合并这些策略可以提升匹配数量。尽管稍微降低了匹配的准确率，但总体上也可以提高召回率和F值。相对而言，MultiOM进一步提高了比对的召回率和F值，因为利用了结构信息，进一步改进概念的向量表示可以获取更多相似的概念并发现更多潜在的匹配，特别是，MultiOM的性能优于MultiOM-，主要原因是采用结构关系有助于区分概念的向量表示。


\indent 表\ref{tab:result2}展示了MultiOM分别将不同视角的模型组合后的效果对比图。总体而言，将不同视角模型的匹配结果合并后，匹配效果会更好。对于上下文视角模模块，就F值而言，改进的TF-IDF（表示为OM-L）比原始TF-IDF更好，这是因为采用表示学习方法将单词表示成向量形式比字符串表示可以提供更多的语义信息。对于外部资源视角嵌入模块（称为OM-R），ConvE和MuitiOM中预训练模型得到的效果优于随机初始化的效果，因为这两种方式都可以利用本体的结构关系在得到更好的概念表示。但是，函数7所构成的预训练模型需要20分钟，ConvE却需要将近24个小时来获得概念的向量表示。值得注意的是，TransE不合适作为本任务的预训练模型对概念进行表示。通过TransE得到的表示并未能够很好的反映概念的结构信息。总体而言，通过数据对比可知在多视角的三个模块中使用新的负采样策略（即OM-S，OM-RS，MultiOM）来融入更多的本体结构信息有助于进一步提高匹配的准确率和F值。

\indent 表\ref{tab:result3}说明了不同视角的预训练模型效果是相互补充的。由于OM-L可以获得比其他映射更多的映射，因此与OM-S，OM-R及其合并的情况相比，可以找到更多的正确映射，因此，也进一步验证了采用多视角的方式来实现本体匹配是有效的。



\indent 表\ref{tab:result4}列出了MultiOM与OAEI2018顶级系统和基于表示学习的SCBOW+DAE模型的效果比较。通过数据表明，就F1-measure而言，MultiOM的初步结果可以与几种有前途的匹配系统（例如FCAMapX和SANOM）竞争。但是，与最佳系统（例如AML和SCBOW+DAE）相比，仍然存在差距。但以上的工作说明，考虑本体的结构信息，以多视角的方式来实现本体的匹配时可行的，为进一步探究其原因，我们进行了下一步实验。
}

\section{基于HORD算法对MultiOM的改进模型MultiOM+HORD的评估结果}
{\songti\xiaosihao 本文选择多种策略来构建基准值以进一步验证MultiOM+HORD模型的有效性。以详细介绍本文实验策略的构造细节，其中，基于对比实验中，基于字符串类方法的名称（包括：StringEquiv，StringEquiv-N，StringEquiv-S，StringEquiv-SR，StringEquiv-NS，StringEquiv-NSR）本文依然沿用MuitiOM模型评估结果中的记号，这里不再赘述。同样，在MuitiOM模型基础上，本文对基于外部资源视角和基于结构视角的预训练模型的超参数优化方式进行了改进，所以，实验中对该部分方法的记号作如下调整：

\indent -OM-L：MultiOM+HORD未对基于上下文视角的预训练模型进行超参优化，所以依然沿用OM-L表示改进后的TF-IDF算法的效果；

\indent -OMH-R：MultiOM中采用OM-R表示基于外部资源视角的预训练模型的匹配结果，MultiOM+HORD采用HORD算法对其参数进行优化，故这里简记为OMH-R；

\indent -OMH-S：MultiOM中采用OM-S表示基于结构视角的预训练模型的匹配结果，MultiOM+HORD采用HORD算法对其参数进行优化，故这里简记为OMH-S；

\indent -OMH-RS：是OMH-R与OMH-S采用多视角匹配算法进行组合后的结果；

\indent -OMH-SR$^-$：未采用本文改进的结构负采样，将OMH-R与OMH-S 采用多视角匹配算法进行组合后的结果；

\indent -MultiOMH：以同样的方式，这里采用MultiOMH表示HORD算法改进MultiOM后的整体结果；MultiOMH$^-$表示MultiOM未采用本文改进的结构负采样的结果。

\indent 实验参数部分，MultiOM+HORD的参数设置与MultiOM方法的有所不同，需要调节的参数MultiOM+HORD不会设定具体的值，只是将其限定在一定的范围内，对未进行优化的参数，MultiOM+HORD依然沿用MultiOM中的参数设置。同时，MultiOM模型本身的优化方法，这里也不做改变，依然采用随机梯度下降（SGD）对MultiOM中的表示学习模型ExterRlModel和StructRlModel的参数进行学习。

\indent MuitiOM中的表示学习模型ExterRlModel和StructRlModel的超参数包括训练轮数（Epoch），学习率（Learning rate），向量维数（Dimensions），以及批次数（Nbatchs），以及负采样率(Negative samping rate)。为降低参数调节的计算量，提升MultiOM+HORD模型效率，本文未对以上六种参数都进行优化。首先，MuitiOM中的表示学习模型的参数规模由向量维数大致确定，而参数的规模应与本体数据集中的概念以及关系的多少有关，在二者数量不变的前提下，可大致确定向量的维数（Dimensions），保证可以适用于匹配任务即可。所以，MuitiOM中未对这两个参数进行调节；其次，训练轮数（Epoch）根据人工经验得到的值为1000，该参数决定了模型学习时间的长短，为保证参数优化后，可以提升MuitiOM 的的效率，我们将其固定为500，通过调节剩余的三个参数：学习率（Learning rate），批次数（Nbatchs）和负采样率(Negative samping rate)以期望以更高的效率获得当前或优于当前的匹配效果。

\indent 其中，MultiOM+HORD将学习率的范围控制在$\left[ {0.001,0.05} \right]$区间内；SGD的批次数（Nbatchs）设置在范围$\left[ {10,60} \right]$ 内；负采样率(Negative samping rate)限制在范围$\left[ {3,15} \right]$中。多视角匹配算法中的阈值也保持不变。

}\\

\subsection{预测结果与分析}


{\songti\xiaosihao 表\ref{tab:result5}列出了MultiOMH与MultiOM以及各种基准匹配结果的对比图。通过对比可以发现，采用HORD算法进行超参优化后MultiOMH 的效果较MultiOM无论是准确率，还是召回率都较之前有所提升，MultiOMH-的性能也优于MultiOM-，并且MultiOMH的结果可以与StringEquiv-NSR相当。MultiOMH是基于MultiOM采用HORD算法进行超参优化的模型，也进一步验证了表示学习模型超参数会影响模型所学特征，显然，经过优化后的参数，可以学到更加准确的特征；另外，无论是MultiOM与MultiOM-还是MultiOMH与MultiOMH-都说明同利用了改进后的结构负采样，效果依然会有所提高，也进一步验证了本体的结构信息有助于本体匹配任务效果的提升。

\indent 表\ref{tab:result6}展示了MultiOMH与MultiOM分别将不同视角的模型组合后的效果对比图。通过各个视角模型的匹配结果详细对比可以发现，采用HORD进行参数优化后，基本上对各个视角的匹配结果都有所提升。众多的结果说明，经过参数优化后，不仅可以使得各个模型错误的匹配得以减少同时可以寻找到一些正确匹配，以上结果进一步说明参数优化后可以帮助表示学习模型学得更加合适得特征，获得得概念向量表示更加有效。

\indent 但以上结果较一些比较好的本体匹配系统得效果依然存在较大差距，通过两个实验结果综合对比可以发现，利用改进得TF-IDF算法的效果准确率有$0.991$,其召回率不高，主要是因为我们利用的是预训练词向量，只包含其字面语义信息，并未对这些预训练词向量基于当前语境进行改进，所以而未包含该本体匹配任务下的语义信息。考虑到以上原因，本文进一步在MultiOM框架下，采用BERT模型进行改进，我们将改进后的模型命名为BertMultiOM，BertMultiOM与MultiOM，OAEI2018顶级系统以及基于表示学习的SCBOW+DAE模型的效果比较见表\ref{tab:result7}。


\indent 根据表\ref{tab:result7}中的结果对比可以发现，BertMultiOM的匹配效果仅仅次于SCBOW+DAE模型的效果，并且较SCBOW+DAE模型，可以找到更多正确匹配；与第二名LogMapBio系统相比，BertMultiOM的正确率与F值要优于LogMapBio，因此，通过该方式对MultiOM的改进是有效的。之前的实验已经验证了HORD算法通过参数优化可以提升MultiOM的匹配效果，而使用了BERT模型后，效果虽然可以提升，但模型的效果依然受到参数的影响，故采用HORD算法进一步对BertMultiOM模型的超参进行调节。因考虑到BertMultiOM模型训练的时间成本，故本节只对BertMultiOM模型的学习率进行优化，改进BertMultiOM学习率后的效果对比如图\ref{tab:result8}所示，BertMultiOM+HORD的匹配效果较参数优化前的BertMultiOM要略好，在数据集FMA-NCI-small中，也近似得到了第一名SCBOW+DAE模型的效果。以上实验进一步说明对表示学习模型的超参数进行优化，使得表示学习模型学到更优的特征，概念的向量表示也更加合适，同时也有助于提升本体匹配模型MultiOM的性能。
}\\
\ifx\allfiles\undefined
\end{document}
\fi
