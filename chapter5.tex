\ifx\allfiles\undefined
\documentclass[a4paper,12.15pt,CJK,oneside]{article}
\begin{document}
%\pagestyle{plain}
\else
\fi

\chapter{实验与评估}
\thispagestyle{fancy} \fancyhead[L]{\songti \wuhao
重庆师范大学硕士学位论文}\fancyhead[R]{\songti \wuhao 4~实验与评估}

\section{评估指标与数据集}

\subsection{数据集介绍}
 {\songti\xiaosihao 本节简要概述在本体匹配实验中使用的四种本体，其中两个本体FMA(Foundational Model of Anatomy)和MA(Adult Mouse
anatomical)分别表示解剖学本体和成年小鼠解剖学本体，二者都是纯粹的解剖学本体论，而另外两个SNOMED CT和NCI是涉及更广的生物医学本体，解剖学只是它们描述的一个子领域\cite{}。虽然这些本体的最新版本是可用的，但是本文在整个本体匹配任务中参考了出现在OAEI(Ontology Alignment Evaluation Initiative) 中的版本，以便在本体匹配系统之间进行比较。

\indent FMA(Foundational Model of Anatomy)：这是一个一直被更新的本体，自1994年由华盛顿大学开发并维护\cite{}，其目的是以机器可读的形式概念化人体的表型结构。

\indent MA(Adult Mouse Anatomical Dictionary)：该本体是描述成年小鼠解剖结构的结构的词汇表\cite{}。

\indent NCI(NCI Thesaurus)：该本体提供了癌症的标准词汇\cite{}，其解剖学子域描述人类自有的生物结构、液体和物质。

\indent SNOMED(SNOMED Clinical Terms)：该本体是一个系统化，组织化的机器可读的医学术语集合，提供临床文档和报告中使用的代码、术语、同义词和定义\cite{}。

\indent 以上本体的概念及对应匹配数量具体如图\ref{tab:dataset}：
\begin{table}[h]
\centering
\caption{本体概念数及对应匹配数量}
\label{tab:dataset}
\begin{tabular}{ccccc}
\hline
 源本体&概念数&目标本体&概念数&匹配数\\
\hline
    MA&2744&NCI&3304&1489\\
\hline
 FMA&3696&NCI&6488&2504\\
\hline
  FMA&10157&SNOMED&13412&7774\\
\hline
\end{tabular}
\end{table}
 
\subsection{评估指标介绍}
  {\songti\xiaosihao 
  OAEI为本体匹配提供了统一的评估标准，评估的指标主要是：匹配的准确率，召回率和F1值。为阐述以上三个指标含义，首先介绍以下概念：
  
  \indent      TP(True Positives) ：表示模型将正例预测为正例的数据条数；
  
  \indent      FP(False Positives)：表示模型将负例预测为正例的数据条数；
  
  \indent      FN(False Negatives)：表示模型将正例预测为负例的数据条数；
  
  \indent      TN(True Negatives) ：表示模型将负例预测为负例的数据条数；
  
\indent 所以，由以上四个参数值，可根据以下公式求得具体指标的值，其中total表示待预测数据的总数。
  
  \indent     准确率(acc)：$acc = \frac{{TP + TN}}{{{\rm{total}}}}$
  
  \indent     召回率(rec)：$rec = \frac{{TP}}{{TP + FN}}$
  
  \indent     F1       值：$F1 = \frac{{2 * acc * rec}}{{acc + rec}}$}\\

\section{基于多视角的本体匹配模型MuitiOM的评估结果}

\subsection{实验环境}
    {\songti\xiaosihao 为了验证MultiOM的有效性，本文使用Python借助TensorFlow\footnote{https://www.tensorflow.org/}（一种非常流行的深度学习框架）来评估MultiOM模型的效果。采用OWLAPI\footnote{http://owlapi.sourceforge.net/}（用于管理OWL本体的工具）本体解析工具来获得本体信息。实验是在具有64GB内存和TiTAN XP GPU的Intel Xeon E5-2630 V4 CPU的个人工作站上进行的。本文的方法，数据集和结果可一起由该链接\footnote{https://github.com/chunyedxx/MultiOM}下载。}
    
\subsection{实验设置}
{\songti\xiaosihao 本文选择几种策略来构建基准值以验证MultiOM模型的有效性。以下是本文实验策略的构造细节，其中N代表Normalization，对概念进行标准化；S表示Synonym，利用同义词词典扩展当前概念；R表示Reference Ontology，使用第三方本体。

\indent -StringEquiv：基于本体中实体名称的字符串等价的基础匹配器。

\indent -StringEquiv-N：先对概念规范化，在进行StringEquiv匹配。

\indent -StringEquiv-S：通过同义词典扩展了概念同义词，在进行StringEquiv匹配。

\indent -StringEquiv-SR：在StringEquiv-S基础上，引入第三方本体实现概念的映射。

\indent -StringEquiv-NS：通过同义词典扩展概念同义词,再进行StringEquiv-N匹配。

\indent -StringEquiv-NSR：对概念的字符串表示进行规范化后，进行StringEquiv-SR匹配。

\indent MultiOM训练时，使用随机梯度下降（SGD）作为优化方法，超参数的配置如下所示：概念和矩阵的维度设置为${\rm{d=\{ 50,100\}}}$和$ {{\rm{d}}_M}{\rm{ = }}\{ {\rm{50}} \times {\rm{50}},{\rm{100}} \times {\rm{100}}\}$。SGD的批次数设置为${\rm{Nbatch=\{ 5,10}},20,50\}$。学习率$r$可选参数有${\rm{\{ 0}}{\rm{.01,0}}{\rm{.02,0}}{\rm{.001\} }}$。对于每个正例，负采样率可选的有${\rm{\{ 1,3,5,10\} }}$。训练的轮数（epoch）设置为1000。在基于上下文模块中，本体概念中单词向量表示主要来自文献\cite{}的链接\footnote{https://doi.org/10.5281/zenodo.1173936}，其维数设置为200。 对于一些没有向量表示的单词，我们将其随机初始化，并满足${\left|{\left|{{t_{1i}}}\right|}\right|_2}{\rm{ }} \le {\rm{ }}1$和${\left|{\left|{{t_{2i}}}\right|}\right|_2}{\rm{ }} \le {\rm{ }}1$。在外部资源视角的模块中，我们使用TransE\cite{}，ConvE\cite{}和预训练函数7初始化第三方本体中概念的向量表示。在损失函数7中将$a$设置为${\rm{\{ 0}}{\rm{.01,0}}{\rm{.05,0}}{\rm{.10\} }}$，以控制概念向量的语义距离。对于负采样策略，我们利用本体中概念的结构信息。当针对一个概念进行负采样替换时，我们参照这个概念的结构关系信息，根据剩余概念与该概念的结构关系进行替换。负采样替换时，与该概念存在“disjointwith”关系的概念优先级最高，并且排除与概念存在“SubClassOf”关系和“PartOF”关系的概念。最后，通过多视角的组合匹配策略生成MultiOM最终匹配结果，并将相关阈值设置为${\delta _1} = 0.8$，${\delta _2} = 0.95$, ${\delta _1} = 0.65$, ${\delta _4} = 0.3$。
为突出我们提出的负采样的效果，在模块标签或合并标签上添加符号“-”表示该模块未配备针对结构关系量身定制的负采样。}\\

\subsection{预测结果与分析}
{\songti\xiaosihao 表\ref{tab:result1}列出了MultiOM与各种基准值的匹配结果。通过对比可以发现，合并这些策略可以提升匹配数量。尽管稍微降低了匹配的准确率，但总体上也可以提高召回率和F值。相对而言，MultiOM进一步提高了比对的召回率和F值，因为利用了结构信息，进一步改进概念的向量表示可以获取更多相似的概念并发现更多潜在的匹配，特别是，MultiOM的性能优于MultiOM-，主要原因是采用结构关系有助于区分概念的向量表示。

\begin{table}[!h]
\centering
\includegraphics[scale=0.6]{result1.png}
\caption{MultiOM与基准方法的比较}
\label{tab:result1}
\end{table}

\indent 表\ref{tab:result2}展示了将不同视角的预训练模型进行组合后效果的比较。总体而言，将的不同视角模型的匹配结果合并后，匹配效果会更好。对于上下文视角模模块，就F值而言，软化的TF-IDF（表示为OM-L）比原始TF-IDF更好，这是因为采用表示学习方法将单词表示成向量形式比字符串表示可以提供更多的语义信息。对于外部资源视角嵌入模块（称为OM-R），ConvE和MuitiOM中预训练模型得到的效果优于随机初始化的效果，因为这两种方式都可以利用本体的结构关系在得到更好的概念表示。但是，函数7所构成的预训练模型需要20分钟，ConvE却需要将近24个小时来获得概念的向量表示。值得注意的是，TransE不合适作为本任务的预训练模型对概念进行表示。通过TransE得到的表示并未能够很好的反映概念的结构信息。总体而言，通过数据对比可知在多视角的三个模块中使用新的负采样策略（即OM-S，OM-RS18 19，MultiOM）有助于进一步提高匹配的准确率和F值。
\begin{table}[!h]
\centering
\includegraphics[scale=0.6]{result2.png}
\caption{不同视角模块组合的结果}
\label{tab:result2}
\end{table}

\begin{table}[!h]
\centering
\includegraphics[scale=0.6]{result3.png}
\caption{不同视角发现的额外匹配}
\label{tab:result3}
\end{table}
表\ref{tab:result3}说明了不同的嵌入视图模块是相互补充的。由于OM-L可以获得比其他映射更多的映射，因此与OM-S，OM-R及其合并的情况相比，可以找到更多的正确映射。
表4列出了MultiOM与基于特征工程的OAEI2018顶级系统和基于表示学习的SCBOW+DAE（O）的比较。它表明，就F1-measure而言，MultiOM的初步结果可以与几种有前途的匹配系统（例如FCAMapX和SANOM）竞争。但是，与最佳系统（例如AML和SCBOW+DAE（O））相比，仍然存在差距。我们分析了基于词法的模块和简化的组合策略可能成为MultiOM的主要瓶颈。得益于叙词表（例如uMlS）和优化的组合策略，大多数排名靠前的系统都能获得更好的OM任务性能。另外，大多数系统（例如AML，LogMap）使用对齐调试技术，这有助于进一步提高对齐质量。但是我们在当前版本中没有采用这些技术。我们将这些问题留在未来的工作中。


}\\
\section{基于多视角的生物医学本体匹配模型--MuitiOM的评估结果}
    {\songti\xiaosihao 待完善}\\
    \subsection{实验设置}
        {\songti\xiaosihao 待完善}\\
    \subsection{本体匹配结果与分析}
        {\songti\xiaosihao 待完善}\\
\section{基于BERT模型和HORD算法对MuitiOM的改进的评估结果}
    {\songti\xiaosihao 待完善}\\
    \subsection{实验设置}
        {\songti\xiaosihao 待完善}\\
    \subsection{本体匹配结果与分析}
        {\songti\xiaosihao 待完善}\\

\ifx\allfiles\undefined
\end{document}
\fi
