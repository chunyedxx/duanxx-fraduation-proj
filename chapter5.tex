\ifx\allfiles\undefined
\documentclass[a4paper,12.15pt,CJK,oneside]{article}
\begin{document}
%\pagestyle{plain}
\else
\fi

\chapter{实验与评估}
\thispagestyle{fancy} \fancyhead[L]{\songti \wuhao
重庆师范大学硕士学位论文}\fancyhead[R]{\songti \wuhao 4~实验与评估}

\section{评估指标与数据集}

\subsection{数据集介绍}
 {\songti\xiaosihao 本节简要概述在本体匹配实验中使用的四种本体，其中两个本体FMA(Foundational Model of Anatomy)和MA(Adult Mouse
anatomical)分别表示解剖学本体和成年小鼠解剖学本体，二者都是纯粹的解剖学本体论，而另外两个SNOMED CT和NCI是涉及更广的生物医学本体，解剖学只是它们描述的一个子领域\cite{}。虽然这些本体的最新版本是可用的，但是本文在整个本体匹配任务中参考了出现在OAEI(Ontology Alignment Evaluation Initiative) 中的版本，以便在本体匹配系统之间进行比较。

\indent FMA(Foundational Model of Anatomy)：这是一个一直被更新的本体，自1994年由华盛顿大学开发并维护\cite{}，其目的是以机器可读的形式概念化人体的表型结构。

\indent MA(Adult Mouse Anatomical Dictionary)：该本体是描述成年小鼠解剖结构的结构的词汇表\cite{}。

\indent NCI(NCI Thesaurus)：该本体提供了癌症的标准词汇\cite{}，其解剖学子域描述人类自有的生物结构、液体和物质。

\indent SNOMED(SNOMED Clinical Terms)：该本体是一个系统化，组织化的机器可读的医学术语集合，提供临床文档和报告中使用的代码、术语、同义词和定义\cite{}。

\indent 以上本体的概念及对应匹配数量具体如图\ref{tab:dataset}：
\begin{table}[h]
\centering
\caption{本体概念数及对应匹配数量}
\label{tab:dataset}
\begin{tabular}{ccccc}
\hline
 源本体&概念数&目标本体&概念数&匹配数\\
\hline
    MA&2744&NCI&3304&1489\\
\hline
 FMA&3696&NCI&6488&2504\\
\hline
  FMA&10157&SNOMED&13412&7774\\
\hline
\end{tabular}
\end{table}

\subsection{评估指标介绍}
  {\songti\xiaosihao
  OAEI为本体匹配提供了统一的评估标准，评估的指标主要是：匹配的准确率，召回率和F1值。为阐述以上三个指标含义，首先介绍以下概念：

  \indent      TP(True Positives) ：表示模型将正例预测为正例的数据条数；

  \indent      FP(False Positives)：表示模型将负例预测为正例的数据条数；

  \indent      FN(False Negatives)：表示模型将正例预测为负例的数据条数；

  \indent      TN(True Negatives) ：表示模型将负例预测为负例的数据条数；

\indent 所以，由以上四个参数值，可根据以下公式求得具体指标的值，其中total表示待预测数据的总数。

  \indent     准确率(acc)：$acc = \frac{{TP + TN}}{{{\rm{total}}}}$

  \indent     召回率(rec)：$rec = \frac{{TP}}{{TP + FN}}$

  \indent     F1       值：$F1 = \frac{{2 * acc * rec}}{{acc + rec}}$}

\section{基于多视角的本体匹配模型MuitiOM的评估结果}
\subsection{实验环境}
    {\songti\xiaosihao 为了验证MultiOM的有效性，本文使用Python借助TensorFlow\footnote{https://www.tensorflow.org/}（一种非常流行的深度学习框架）来评估MultiOM模型的效果。采用OWLAPI\footnote{http://owlapi.sourceforge.net/}（用于管理OWL本体的工具）本体解析工具来获得本体信息。实验是在具有64GB内存和TiTAN XP GPU的Intel Xeon E5-2630 V4 CPU的个人工作站上进行的。本文的方法，数据集和结果可一起由该链接\footnote{https://github.com/chunyedxx/MultiOM}下载。}

\subsection{实验设置}
{\songti\xiaosihao 本文选择几种策略来构建基准值以验证MultiOM模型的有效性。以下是本文实验策略的构造细节，其中N代表Normalization，对概念进行标准化；S表示Synonym，利用同义词词典扩展当前概念；R表示Reference Ontology，使用第三方本体。

\indent -StringEquiv：基于本体中实体名称的字符串等价的基础匹配器。

\indent -StringEquiv-N：先对概念规范化，在进行StringEquiv匹配。

\indent -StringEquiv-S：通过同义词典扩展了概念同义词，在进行StringEquiv匹配。

\indent -StringEquiv-SR：在StringEquiv-S基础上，引入第三方本体实现概念的映射。

\indent -StringEquiv-NS：通过同义词典扩展概念同义词,再进行StringEquiv-N匹配。

\indent -StringEquiv-NSR：对概念的字符串表示进行规范化后，进行StringEquiv-SR匹配。

\indent MultiOM训练时，使用随机梯度下降（SGD）作为优化方法，超参数的配置如下所示：概念和矩阵的维度设置为${\rm{d=\{ 50,100\}}}$和$ {{\rm{d}}_M}{\rm{ = }}\{ {\rm{50}} \times {\rm{50}},{\rm{100}} \times {\rm{100}}\}$。SGD的批次数设置为${\rm{Nbatch=\{ 5,10}},20,50\}$。学习率$r$可选参数有${\rm{\{ 0}}{\rm{.01,0}}{\rm{.02,0}}{\rm{.001\} }}$。对于每个正例，负采样率可选的有${\rm{\{ 1,3,5,10\} }}$。训练的轮数（epoch）设置为1000。在基于上下文模块中，本体概念中单词向量表示主要来自文献\cite{}的链接\footnote{https://doi.org/10.5281/zenodo.1173936}，其维数设置为200。 对于一些没有向量表示的单词，我们将其随机初始化，并满足${\left|{\left|{{t_{1i}}}\right|}\right|_2}{\rm{ }} \le {\rm{ }}1$和${\left|{\left|{{t_{2i}}}\right|}\right|_2}{\rm{ }} \le {\rm{ }}1$。在外部资源视角的模块中，我们使用TransE\cite{}，ConvE\cite{}和预训练函数7初始化第三方本体中概念的向量表示。在损失函数7中将$a$设置为${\rm{\{ 0}}{\rm{.01,0}}{\rm{.05,0}}{\rm{.10\} }}$，以控制概念向量的语义距离。对于负采样策略，我们利用本体中概念的结构信息。当针对一个概念进行负采样替换时，我们参照这个概念的结构关系信息，根据剩余概念与该概念的结构关系进行替换。负采样替换时，与该概念存在“disjointwith”关系的概念优先级最高，并且排除与概念存在“SubClassOf”关系和“PartOF”关系的概念。最后，通过多视角的组合匹配策略生成MultiOM最终匹配结果，并将相关阈值设置为${\delta _1} = 0.8$，${\delta _2} = 0.95$, ${\delta _1} = 0.65$, ${\delta _4} = 0.3$。
为突出我们提出的负采样的效果，在模块标签或合并标签上添加符号“-”表示该模块未配备针对结构关系量身定制的负采样。}\\

\subsection{预测结果与分析}
{\songti\xiaosihao 表\ref{tab:result1}列出了MultiOM以及各种基准匹配结果的对比图。通过对比可以发现，合并这些策略可以提升匹配数量。尽管稍微降低了匹配的准确率，但总体上也可以提高召回率和F值。相对而言，MultiOM进一步提高了比对的召回率和F值，因为利用了结构信息，进一步改进概念的向量表示可以获取更多相似的概念并发现更多潜在的匹配，特别是，MultiOM的性能优于MultiOM-，主要原因是采用结构关系有助于区分概念的向量表示。

\begin{table}
	\centering
	\footnotesize
	\caption{MultiOM以及各种基准匹配结果的对比图}
	\label{tab:result1}
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
		\multirow{2}{*}{Methods} &\multicolumn{5}{c|}{MA-NCI} & \multicolumn{5}{c|}{FMA-NCI-small} \\
		&Number & Correct& P &R & F1&Number & Correct& P &R & F1\\\hline
		StringEquiv & 935& 932& 0.997  & 0.615 &   0.761
		& 1501 & 1389 & 0.925  &0.517 &0.663   \\\hline
		StringEquiv-N &992 & 989&  0.997& 0.625 & 0.789
		& 1716 & 1598 & 0.931  &0.595 &  0.726 \\\hline
		StringEquiv-S &1100 & 1057& 0.961 & 0.697&  0.808
		& 2343 & 2082 & 0.889  &0.775 &  0.828 \\\hline
		StringEquiv-SR & 1162& 1094 & 0.941 & 0.722 &  0.817
		& 2343 & 2082 & 0.889  &0.775 &  0.828 \\\hline
		StringEquiv-NS & 1153& 1109& 0.962 & 0.732&   0.831
		& 2464 & 2200 & 0.893  &0.819 &  0.854  \\\hline
		StringEquiv-NSR &1211 & 1143  & 0.943  & 0.753 &  0.838
		& 2467 & 2203 & 0.893  &0.820 &  0.855  \\\hline
		MultiOM$^-$&1484 & 1296   & 0.873  & 0.855 &  0.864
		&  2471   &  2192  &  0.887  & 0.809   & 0.846    \\\hline
		MultiOM&1445 & 1287   & 0.891  & 0.849 &  0.869
		&  2470  & 2195  &    0.889 &   0.817 &   0.0.851  \\\hline
	\end{tabular}
\end{table}}

\indent 表\ref{tab:result2}展示了MultiOM分别将不同视角的模型组合后的效果对比图。总体而言，将的不同视角模型的匹配结果合并后，匹配效果会更好。对于上下文视角模模块，就F值而言，改进的TF-IDF（表示为OM-L）比原始TF-IDF更好，这是因为采用表示学习方法将单词表示成向量形式比字符串表示可以提供更多的语义信息。对于外部资源视角嵌入模块（称为OM-R），ConvE和MuitiOM中预训练模型得到的效果优于随机初始化的效果，因为这两种方式都可以利用本体的结构关系在得到更好的概念表示。但是，函数7所构成的预训练模型需要20分钟，ConvE却需要将近24个小时来获得概念的向量表示。值得注意的是，TransE不合适作为本任务的预训练模型对概念进行表示。通过TransE得到的表示并未能够很好的反映概念的结构信息。总体而言，通过数据对比可知在多视角的三个模块中使用新的负采样策略（即OM-S，OM-RS18 19，MultiOM）有助于进一步提高匹配的准确率和F值。

\begin{table}[!h]
	\centering
	\caption{MultiOM分别将不同视角的模型组合后的效果对比图}
	\footnotesize
	\label{tab:result2}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Methods &Number & Correct& P &R & F1\\\hline
		TFIDF (threshold$=0.8$) &  985        &    976    &    0.991         &   0.644        & 0.780     \\\hline
		OM-$L$ (threshold$=0.8$)     & 1286& 1175& 0.914  & 0.775 &   0.839  \\\hline
		OM-$S^-$ (threshold$=0.95$)  & 1836& 1109      &    0.604   & 0.732  & 0.662    \\\hline
		OM-$S$ (threshold$=0.95$)    & 1189& 1097& 0.923  & 0.724 &   0.811  \\\hline
		OM-$R$ (Random initialization, threshold$=0.95$) &    709   &   680       &  0.959   &  0.449   &  0.612      \\\hline
		OM-$R$ (loss function \ref{Pretaining},  threshold$=0.95$)     & 833& 789   & 0.948    & 0.520 &   0.672  \\\hline
		OM-$RS^-$ (threshold$=0.95$)   & 1271& 1147& 0.902  & 0.757 &   0.823  \\\hline
		OM-$RS$ (threshold$=0.95$)   & 1237& 1138& 0.920  & 0.751 &   0.827  \\\hline
        OM-$S^-$ (threshold$=0.95$)  & 1821& 1110      &    0.610   & 0.733  & 0.666    \\\hline
        MultiOM$^-$&1484 & 1296   & 0.873  & 0.855 &  0.864 \\\hline
        MultiOM&1445 & 1287   & 0.891  & 0.849 &  0.869  \\\hline
	\end{tabular}
\end{table}

\begin{table}
	\centering
	\caption{The external correct mappings discovered in each embedding-view module}
	\label{tab:result3}
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		              &OM-$L$ & OM-$S$& OM-$R$ &OM-$LS$ & OM-$LR$ & OM-$SR$ \\\hline
		OM-$L$ &  --  &  176  &   463  & --    &  --     &    154    \\\hline
		OM-$S$ &  99  &   -- &   354  & --   &   45    &  --      \\\hline
		OM-$R$  & 78   & 46   &   -- &   24  &   --    &    --    \\\hline
	\end{tabular}
\end{table}

表\ref{tab:result3}说明了不同的嵌入视图模块是相互补充的。由于OM-L可以获得比其他映射更多的映射，因此与OM-S，OM-R及其合并的情况相比，可以找到更多的正确映射。
表4列出了MultiOM与基于特征工程的OAEI2018顶级系统和基于表示学习的SCBOW+DAE（O）的比较。它表明，就F1-measure而言，MultiOM的初步结果可以与几种有前途的匹配系统（例如FCAMapX和SANOM）竞争。但是，与最佳系统（例如AML和SCBOW+DAE（O））相比，仍然存在差距。我们分析了基于词法的模块和简化的组合策略可能成为MultiOM的主要瓶颈。得益于叙词表（例如uMlS）和优化的组合策略，大多数排名靠前的系统都能获得更好的OM任务性能。另外，大多数系统（例如AML，LogMap）使用对齐调试技术，这有助于进一步提高对齐质量。但是我们在当前版本中没有采用这些技术。我们将这些问题留在未来的工作中。
}

\section{基于HORD算法对MultiOM的改进模型MultiOM+HORD}
{\songti\xiaosihao 本文选择多种策略来构建基准值以进一步验证MultiOM+HORD模型的有效性。以详细介绍本文实验策略的构造细节，其中，基于对比实验中，基于字符串类方法的名称（包括：StringEquiv，StringEquiv-N，StringEquiv-S，StringEquiv-SR，StringEquiv-NS，StringEquiv-NSR）本文依然沿用MuitiOM模型评估结果中的记号，这里不再赘述。同样，在MuitiOM模型基础上，本文对基于外部资源视角和基于结构视角的预训练模型的超参数优化方式进行了改进，所以，实验中对该部分方法的记号作如下调整：
\indent -OM-L：MultiOM+HORD未对基于上下文视角的预训练模型进行超参优化，所以依然沿用OM-L表示改进后的TF-IDF算法的效果；
\indent -OMH-R：MultiOM中采用OM-R表示基于外部资源视角的预训练模型的匹配结果，MultiOM+HORD采用HORD算法对其参数进行优化，故这里简记为OMH-R；
\indent -OMH-S：MultiOM中采用OM-S表示基于结构视角的预训练模型的匹配结果，MultiOM+HORD采用HORD算法对其参数进行优化，故这里简记为OMH-S；
\indent -OMH-RS：是OMH-R与OMH-S采用多视角匹配算法进行组合后的结果；
\indent -OMH-SR^-：未采用本文改进的结构负采样，将OMH-R与OMH-S采用多视角匹配算法进行组合后的结果；
\indent -MultiOM+HORD：以同样的方式，这里采用MultiOMH表示HORD算法改进MultiOM后的整体结果；MultiOM+HORD^-表示MultiOM未采用本文改进的结构负采样的结果。

\indent 实验参数部分，MultiOM+HORD的参数设置与MultiOM方法的有所不同，需要调节的参数MultiOM+HORD不会设定具体的值，只是将其限定在一定的范围内，对未进行优化的参数，MultiOM+HORD依然沿用MultiOM中的参数设置。同时，MultiOM模型本身的优化方法，这里也不做改变，依然采用随机梯度下降（SGD）对MultiOM中的表示学习模型ExterRlModel和StructRlModel的参数进行学习。

\indent MuitiOM中的表示学习模型ExterRlModel和StructRlModel的超参数包括训练轮数（Epoch），学习率（Learning rate），向量维数（Dimensions），以及批次数（Nbatchs），以及负采样率(Negative samping rate)。为降低参数调节的计算量，提升MultiOM+HORD模型效率，本文未对以上六种参数都进行优化。首先，MuitiOM中的表示学习模型的参数规模由向量维数大致确定，而参数的规模应与本体数据集中的概念以及关系的多少有关，在二者数量不变的前提下，可大致确定向量的维数（Dimensions），保证可以适用于匹配任务即可。所以，MuitiOM中未对这两个参数进行调节；其次，训练轮数（Epoch）根据人工经验得到的值为1000，该参数决定了模型学习时间的长短，为保证参数优化后，可以提升MuitiOM 的的效率，我们将其固定为500，通过调节剩余的三个参数：学习率（Learning rate），批次数（Nbatchs）和负采样率(Negative samping rate)以期望以更高的效率获得当前或优于当前的匹配效果。

\indent 其中，MultiOM+HORD将学习率的范围控制在$\left[ {0.001:0.05} \right]$区间内；SGD的批次数（Nbatchs）设置在范围$\left[ {10:60} \right]$内；负采样率(Negative samping rate)限制在范围$\left[ {3:15} \right]$中。多视角匹配算法中的阈值也保持不变。

}\\

\subsection{预测结果与分析}
{\songti\xiaosihao 表\ref{tab:result5}列出了MultiOMH与MultiOM以及各种基准匹配结果的对比图。通过对比可以发现，采用HORD算法进行超参优化后MultiOMH 的效果较MultiOM无论是准确率，还是召回率都较之前有所提升，MultiOMH-的性能也优于MultiOM-，并且MultiOMH的结果可以与StringEquiv-NSR相当。MultiOMH是基于MultiOM采用HORD算法进行超参优化的模型，也进一步验证了表示学习模型超参数会影响模型所学特征，显然，经过优化后的参数，可以学到更加准确的特征；另外，无论是MultiOM与MultiOM-还是MultiOMH与MultiOMH-都说明同利用了改进后的结构负采样，效果依然会有所提高，也进一步验证了本体的结构信息有助于本体匹配任务效果的提升。

\begin{table}
	\centering
	\footnotesize
	\caption{MultiOMH与MultiOM以及各种基准匹配结果的对比图}
	\label{tab:result5}
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
		\multirow{2}{*}{Methods} &\multicolumn{5}{c|}{MA-NCI} & \multicolumn{5}{c|}{FMA-NCI-small} \\
		&Number & Correct& P &R & F1&Number & Correct& P &R & F1\\\hline
		StringEquiv & 935& 932& 0.997  & 0.615 &   0.761
		& 1501 & 1389 & 0.925  &0.517 &0.663   \\\hline
		StringEquiv-N &992 & 989&  0.997& 0.625 & 0.789
		& 1716 & 1598 & 0.931  &0.595 &  0.726 \\\hline
		StringEquiv-S &1100 & 1057& 0.961 & 0.697&  0.808
		& 2343 & 2082 & 0.889  &0.775 &  0.828 \\\hline
		StringEquiv-SR & 1162& 1094 & 0.941 & 0.722 &  0.817
		& 2343 & 2082 & 0.889  &0.775 &  0.828 \\\hline
		StringEquiv-NS & 1153& 1109& 0.962 & 0.732&   0.831
		& 2464 & 2200 & 0.893  &0.819 &  0.854  \\\hline
		StringEquiv-NSR &1211 & 1143  & 0.943  & 0.753 &  0.838
		& 2467 & 2203 & 0.893  &0.820 &  0.855  \\\hline
		MultiOM$^-$&1484 & 1296   & 0.873  & 0.855 &  0.864
		&  2471   &  2192  &  0.887  & 0.809   & 0.846    \\\hline
		MultiOM&1445 & 1287   & 0.891  & 0.849 &  0.869
		&  2470  & 2195  &    0.889 &   0.817 &   0.0.851  \\\hline
        MultiOMH$^-$&1462 & 1296   & 0.886  & 0.855 &  0.870
		&  2469   &  2192  &  0.888  & 0.809   & 0.847    \\\hline
        MultiOMH&1431 & 1290   & 0.901  & 0.851 &  0.0.876
		&  2466  & 2200  &    0.892 &   0.819 &   0.854  \\\hline
	\end{tabular}
\end{table}}

\indent 表\ref{tab:result2}展示了MultiOMH与MultiOM分别将不同视角的模型组合后的效果对比图。总体而言，将的不同视角模型的匹配结果合并后，匹配效果会更好。对于上下文视角模模块，就F值而言，改进的TF-IDF（表示为OM-L）比原始TF-IDF更好，这是因为采用表示学习方法将单词表示成向量形式比字符串表示可以提供更多的语义信息。对于外部资源视角嵌入模块（称为OM-R），ConvE和MuitiOM中预训练模型得到的效果优于随机初始化的效果，因为这两种方式都可以利用本体的结构关系在得到更好的概念表示。但是，函数7所构成的预训练模型需要20分钟，ConvE却需要将近24个小时来获得概念的向量表示。值得注意的是，TransE不合适作为本任务的预训练模型对概念进行表示。通过TransE得到的表示并未能够很好的反映概念的结构信息。总体而言，通过数据对比可知在多视角的三个模块中使用新的负采样策略（即OM-S，OM-RS18 19，MultiOM）有助于进一步提高匹配的准确率和F值。

\begin{table}
	\centering
	\caption{MultiOMH与MultiOM分别将不同视角的模型组合后的效果对比图}
	\footnotesize
	\label{Combination}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Methods &Number & Correct& P &R & F1\\\hline
		OM-$S^-$ (threshold$=0.95$)  & 1836& 1109      &    0.604   & 0.732  & 0.662    \\\hline
		OM-$S$ (threshold$=0.95$)    & 1189& 1097& 0.923  & 0.724 &   0.811  \\\hline
		OM-$R$ (Random initialization, threshold$=0.95$) &    709   &   680       &  0.959   &  0.449   &  0.612      \\\hline
		OM-$R$ (loss function \ref{Pretaining},  threshold$=0.95$)     & 833& 789   & 0.948    & 0.520 &   0.672  \\\hline
		OM-$RS^-$ (threshold$=0.95$)   & 1271& 1147& 0.902  & 0.757 &   0.823  \\\hline
		OM-$RS$ (threshold$=0.95$)   & 1237& 1138& 0.920  & 0.751 &   0.827  \\\hline
        MultiOM$^-$&1484 & 1296   & 0.873  & 0.855 &  0.864 \\\hline
        MultiOM&1445 & 1287   & 0.891  & 0.849 &  0.869  \\\hline
        OMH-$S^-$ (threshold$=0.95$)  & 1821& 1110      &    0.610   & 0.733  & 0.666    \\\hline
		OMH-$S$ (threshold$=0.95$)    & 1170& 1099& 0.939  & 0.725 &   0.818  \\\hline
		OMH-$R$ (Random initialization, threshold$=0.95$) &    705   &   680       &  0.965   &  0.449   &  0.613      \\\hline
		OMH-$R$ (loss function \ref{Pretaining},  threshold$=0.95$)     & 824& 794   & 0.964    & 0.524 &   0.679  \\\hline
		OMH-$RS^-$ (threshold$=0.95$)   & 1263& 1147& 0.908  & 0.757 &   0.826  \\\hline
		OMH-$RS$ (threshold$=0.95$)   & 1221& 1140& 0.934  & 0.752 &   0.833  \\\hline
		MultiOMH$^-$ &1462 & 1296   & 0.873  & 0.855 &  0.864  \\\hline
		MultiOMH&1431 & 1290   & 0.901  & 0.851 &  0.876  \\\hline
	\end{tabular}
\end{table}

}\\
    \subsection{实验设置}
        {\songti\xiaosihao 待完善}\\
    \subsection{本体匹配结果与分析}
        {\songti\xiaosihao 待完善}\\
\section{基于BERT模型和HORD算法对MuitiOM的改进的评估结果}
    {\songti\xiaosihao 待完善}\\
    \subsection{实验设置}
        {\songti\xiaosihao 待完善}\\
    \subsection{本体匹配结果与分析}
        {\songti\xiaosihao 待完善}\\

\ifx\allfiles\undefined
\end{document}
\fi
