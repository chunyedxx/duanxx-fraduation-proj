\ifx\allfiles\undefined
\documentclass[a4paper,12.15pt,CJK,oneside]{article}
\begin{document}
%\pagestyle{plain}
\else
\fi

\chapter{实验与评估}
\thispagestyle{fancy} \fancyhead[L]{\songti \wuhao
重庆师范大学硕士学位论文}\fancyhead[R]{\songti \wuhao 4~实验与评估}

\section{评估指标与数据集}

\subsection{数据集}
{\songti\xiaosihao 本节简要概述在本体匹配实验中使用的四种本体，其中两个本体FMA(Foundational Model of Anatomy)和MA(Adult Mouse
anatomical)分别表示解剖学本体和成年小鼠解剖学本体，二者都是纯粹的解剖学本体论，而另外两个SNOMED CT和NCI是涉及更广的生物医学本体，解剖学只是它们描述的一个子领域\cite{}。虽然这些本体的最新版本是可用的，但是本文在整个本体匹配任务中参考了出现在OAEI(Ontology Alignment Evaluation Initiative) 中的版本，以便在本体匹配系统之间进行比较。

\indent FMA(Foundational Model of Anatomy)：这是一个一直被更新的本体，自1994年由华盛顿大学开发并维护\cite{}，其目的是以机器可读的形式概念化人体的表型结构。

\indent MA(Adult Mouse Anatomical Dictionary)：该本体是描述成年小鼠解剖结构的结构的词汇表\cite{}。

\indent NCI(NCI Thesaurus)：该本体提供了癌症的标准词汇\cite{}，其解剖学子域描述人类自有的生物结构、液体和物质。

\indent SNOMED(SNOMED Clinical Terms)：该本体是一个系统化，组织化的机器可读的医学术语集合，提供临床文档和报告中使用的代码、术语、同义词和定义\cite{}。

\indent 以上本体的概念及对应匹配数量具体如图\ref{tab:dataset}：
\begin{table}[h]
\centering
\caption{本体概念数及对应匹配数量}
\label{tab:dataset}
\begin{tabular}{ccccc}
\hline
 源本体&概念数&目标本体&概念数&匹配数\\
\hline
    MA&2744&NCI&3304&1489\\
\hline
 FMA&3696&NCI&6488&2504\\
\hline
  FMA&10157&SNOMED&13412&7774\\
\hline
\end{tabular}
\end{table}

\subsection{评估指标}
  {\songti\xiaosihao
  OAEI为本体匹配提供了统一的评估标准，评估的指标主要是：匹配的准确率，召回率和F1值。为阐述以上三个指标含义，首先介绍以下概念：

  \indent      TP(True Positives) ：表示模型将正例预测为正例的数据条数；

  \indent      FP(False Positives)：表示模型将负例预测为正例的数据条数；

  \indent      FN(False Negatives)：表示模型将正例预测为负例的数据条数；

  \indent      TN(True Negatives) ：表示模型将负例预测为负例的数据条数；

\indent 所以，由以上四个参数值，可根据以下公式求得具体指标的值，其中total表示待预测数据的总数。

  \indent     准确率(acc)：$acc = \frac{{TP + TN}}{{{\rm{total}}}}$

  \indent     召回率(rec)：$rec = \frac{{TP}}{{TP + FN}}$

  \indent     F1       值：$F1 = \frac{{2 * acc * rec}}{{acc + rec}}$}


\section{实验环境}
{\songti\xiaosihao 为了验证MultiOM的有效性，本文使用Python借助TensorFlow\footnote{https://www.tensorflow.org/}（一种非常流行的深度学习框架）来评估MultiOM模型的效果。采用OWLAPI\footnote{http://owlapi.sourceforge.net/}（用于管理OWL本体的工具）本体解析工具来获得本体信息。实验是在具有64GB内存和TiTAN XP GPU的Intel Xeon E5-2630 V4 CPU的个人工作站上进行的。本文的方法，数据集和结果可一起由该链接\footnote{https://github.com/chunyedxx/MultiOM}下载。}

\section{实验结果}

本体概念中单词向量表示主要来自文献\cite{}的链接\footnote{https://doi.org/10.5281/zenodo.1173936}，

其维数设置为200。 

对于一些没有向量表示的单词，我们将其随机初始化，并满足${\left|{\left|{{t_{1i}}}\right|}\right|_2}{\rm{ }} \le {\rm{ }}1$ 和${\left|{\left|{{t_{2i}}}\right|}\right|_2}{\rm{ }} \le {\rm{ }}1$。

{\songti\xiaosihao 针对现有方法的两个问题，本文设计如下实验，逐步验证本文建模思想是否正确，是否可以有效解决现有方法的两个问题，具体实验步骤由三部分构成，分别是采用本体结构信息的MOM方法与基于字符串方法的对比实验，MOM多视角结合与单个预训练模型的结果对比，MOMWH与MOM结果对比以及MOMWH$^+$与MOMWH,MOM及其他方法结果对比。}

\subsection{采用本体结构信息的MOM方法与基于字符串方法的对比实验}
{\songti\xiaosihao 本实验采用MA-NCI-FMA和FMA-NCI-small-SNOMED两个数据集，将本文的MOM，MOM$^-$方法与StringEquiv，StringEquiv-N，StringEquiv-S，StringEquiv-SR，StringEquiv-NS，StringEquiv-NSR进行对比，以上方法中唯一的变量因素为有没有使用本体的结构信息，通过该实验来验证本体的结构信息是否有助于提升本体匹配的效果。其中，MOM和MOM$^-$方法训练时共享同组参数，具体参数如表\ref{tab:canhsu}所示：

\begin{table}[!h]
\label{tab:canhsu}
\caption{设置宽度}
\centering
\begin{tabular}{cc}
\toprule  %添加表格头部粗线
\midrule  %添加表格中横线
参数     & 参数值 \\
\hline
$d$      & $\{{\rm{50}},{\rm{100}}\}$ \\
$d_M$    & $\{ {\rm{50}} \times {\rm{50}},{\rm{100}} \times {\rm{100}}\}$ \\
$nbatch$ & $\{ {\rm{5}},{\rm{10}},20,50\}$\\
$r$      & $\{ {\rm{0}}.{\rm{01}},{\rm{0}}.{\rm{02}},{\rm{0}}.{\rm{001}}\}$\\
$nrate$  & $\{ {\rm{1}},{\rm{3}},{\rm{5}},{\rm{10}}\}$\\
$epoch$  & $1000$\\
$a$      & $\{ {\rm{0}}.{\rm{01}},{\rm{0}}.{\rm{05}},{\rm{0}}.{\rm{10}}\}$\\
$\{ {\delta _1},{\delta _2},{\delta _3},{\delta _4}\}$ & $\{ 0.8,0.95,0.65,0.3\}$\\
\bottomrule %添加表格底部粗线
\hline
\end{tabular}
\end{table}

\indent 实验结果如图\ref{tab:result1}所示，列出了MultiOM以及各种基准匹配结果的对比图。通过对比可以发现，合并这些策略可以提升匹配数量。尽管稍微降低了匹配的准确率，但总体上也可以提高召回率和F值。相对而言，MultiOM进一步提高了比对的召回率和F 值，因为利用了结构信息，进一步改进概念的向量表示可以获取更多相似的概念并发现更多潜在的匹配，特别是，MultiOM的性能优于MultiOM-，主要原因是采用结构关系有助于区分概念的向量表示。


\begin{table}[!h]
	\centering
	\footnotesize
	\caption{MultiOM以及各种基准匹配结果的对比图}
	\label{tab:result1}
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
		\multirow{Methods} &\multicolumn{5}{c|}{MA-NCI} & \multicolumn{5}{c|}{FMA-NCI-small} \\
		&Number & Correct& P &R & F1&Number & Correct& P &R & F1\\\hline
		StringEquiv & 935& 932& 0.997  & 0.615 &   0.761
		& 1501 & 1389 & 0.925  &0.517 &0.663   \\\hline
		StringEquiv-N &992 & 989&  0.997& 0.625 & 0.789
		& 1716 & 1598 & 0.931  &0.595 &  0.726 \\\hline
		StringEquiv-S &1100 & 1057& 0.961 & 0.697&  0.808
		& 2343 & 2082 & 0.889  &0.775 &  0.828 \\\hline
		StringEquiv-SR & 1162& 1094 & 0.941 & 0.722 &  0.817
		& 2343 & 2082 & 0.889  &0.775 &  0.828 \\\hline
		StringEquiv-NS & 1153& 1109& 0.962 & 0.732&   0.831
		& 2464 & 2200 & 0.893  &0.819 &  0.854  \\\hline
		StringEquiv-NSR &1211 & 1143  & 0.943  & 0.753 &  0.838
		& 2467 & 2203 & 0.893  &0.820 &  0.855  \\\hline
		MOM$^-$&1484 & 1296   & 0.873  & 0.855 &  0.864
		&  2471   &  2192  &  0.887  & 0.809   & 0.846    \\\hline
		MOM&1445 & 1287   & 0.891  & 0.849 &  0.869
		&  2470  & 2195  &    0.889 &   0.817 &   0.0.851  \\\hline
	\end{tabular}
\end{table}}
}
\subsection{MOM多视角结合与单个预训练模型的结果对比}
{\songti\xiaosihao 本实验针对MA-NCI-FMA数据集，将本文的MOM，MOM$^-$，MOM-S，MOM-L，MOM-R，MOM-RS方法进行对比，以上方法中唯一的变量因素为不同的方法对应的视角不同，通过该实验来验证不同的视角是否可以学习到不同方面的特征，多视角的融合是否有助于提升本体匹配的效果。其中，MOM和MOM$^-$ 方法训练时共享同组参数，实验中也对比了对于同一视角的模型在不同的阈值下模型的匹配效果。

\indent 实验结果如图\ref{tab:result2,tab:result3}所示：
\begin{table}[!h]
	\centering
	\caption{MultiOM分别将不同视角的模型组合后的效果对比图}
	\footnotesize
	\label{tab:result2}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Methods &Number & Correct& P &R & F1\\\hline
		TFIDF (threshold$=0.8$) &  985        &    976    &    0.991         &   0.644        & 0.780     \\\hline
		MOM-$L$ (threshold$=0.8$)     & 1286& 1175& 0.914  & 0.775 &   0.839  \\\hline
		MOM-$S^-$ (threshold$=0.95$)  & 1836& 1109      &    0.604   & 0.732  & 0.662    \\\hline
		MOM-$S$ (threshold$=0.95$)    & 1189& 1097& 0.923  & 0.724 &   0.811  \\\hline
		MOM-$R$ (Random initialization, threshold$=0.95$) &    709   &   680       &  0.959   &  0.449   &  0.612      \\\hline
		MOM-$R$ (loss function \ref{Pretaining},  threshold$=0.95$)     & 833& 789   & 0.948    & 0.520 &   0.672  \\\hline
		MOM-$RS^-$ (threshold$=0.95$)   & 1271& 1147& 0.902  & 0.757 &   0.823  \\\hline
		MOM-$RS$ (threshold$=0.95$)   & 1237& 1138& 0.920  & 0.751 &   0.827  \\\hline
        MOM-$S^-$ (threshold$=0.95$)  & 1821& 1110      &    0.610   & 0.733  & 0.666    \\\hline
        MOM$^-$&1484 & 1296   & 0.873  & 0.855 &  0.864 \\\hline
        MOM&1445 & 1287   & 0.891  & 0.849 &  0.869  \\\hline
	\end{tabular}
\end{table}

\begin{table}[!h]
	\centering
	\caption{MultiOM不同视角的模型效果对比}
	\label{tab:result3}
	\begin{tabular}{|c|c|c|c|c|c|c|}
		\hline
		              &OM-$L$ & OM-$S$& OM-$R$ &OM-$LS$ & OM-$LR$ & OM-$SR$ \\\hline
		OM-$L$ &  --  &  176  &   463  & --    &  --     &    154    \\\hline
		OM-$S$ &  99  &   -- &   354  & --   &   45    &  --      \\\hline
		OM-$R$  & 78   & 46   &   -- &   24  &   --    &    --    \\\hline
	\end{tabular}
\end{table}

\indent 表\ref{tab:result2}展示了MultiOM分别将不同视角的模型组合后的效果对比图。总体而言，将不同视角模型的匹配结果合并后，匹配效果会更好。对于上下文视角模模块，就F值而言，改进的TF-IDF（表示为OM-L）比原始TF-IDF更好，这是因为采用表示学习方法将单词表示成向量形式比字符串表示可以提供更多的语义信息。对于外部资源视角嵌入模块（称为OM-R），ConvE和MuitiOM中预训练模型得到的效果优于随机初始化的效果，因为这两种方式都可以利用本体的结构关系在得到更好的概念表示。但是，函数7所构成的预训练模型需要20分钟，ConvE却需要将近24个小时来获得概念的向量表示。值得注意的是，TransE 不合适作为本任务的预训练模型对概念进行表示。通过TransE得到的表示并未能够很好的反映概念的结构信息。总体而言，通过数据对比可知在多视角的三个模块中使用新的负采样策略（即OM-S，OM-RS，MultiOM）来融入更多的本体结构信息有助于进一步提高匹配的准确率和F值。

\indent 表\ref{tab:result3}说明了不同视角的预训练模型效果是相互补充的。由于OM-L可以获得比其他映射更多的映射，因此与OM-S，OM-R及其合并的情况相比，可以找到更多的正确映射，因此，也进一步验证了采用多视角的方式来实现本体匹配是有效的。}


\subsection{MOMWH与MOM结果对比}
{\songti\xiaosihao 本实验针对MA-NCI-FMA和FMA-NCI-small-SNOME两个数据集，将本文的MOMWH，MOMWH$^-$，MOM，MOM$^-$，以及MOMWH和MOM中多个不同视角的模型，将MOMWH与MOM中相对应的模型进行对比，以上方法中唯一的变量因素为MOM中的模型参数是人工调节的参数，MOMWH中模型参数一部分为采用HORD 算法优化得到的参数，通过该实验来验证HORD算法是否可以改进表示学习模型学得的概念向量表示，是否可以因此提升本体匹配的效果。其中，MOM方法训练与其匹配算法的参数如图，MOMWH模型训练时的参数，实验中也对比了对于同一视角的模型在不同的阈值下模型的匹配效果。

\indent 该实验首先需采用HORD算法对MOM中的表示学习模型MOM-S，MOM-R进行超参数优化。HORD算法的执行与MOM-L或MOM-R训练的方式有所不同，需给出模型（如MOM-S，MOM-R类模型）中待优化超参数的区间范围，而不用给定具体的值。对于模型中没有进行进一步优化的参数，则采用MOM中对应模型相同的参数。

\indent MOM-S和MOM-R的超参数包括训练轮数（Epoch），学习率（Learning rate），向量维数（Dimensions），以及批次数（Nbatchs），以及负采样率(Negative samping rate)。为降低参数调节的计算量，提升MOMWH模型效率，本文未对以上六种参数都进行优化。首先，MOM中的表示学习模型的参数规模由向量维数大致确定，而参数的规模应与本体数据集中的概念以及关系的多少有关，在二者数量不变的前提下，可大致确定向量的维数（Dimensions），保证可以适用于匹配任务即可。所以，MOM中未对这两个参数进行调节；其次，训练轮数（Epoch）根据人工经验得到的值为1000，该参数决定了模型学习时间的长短，为保证参数优化后，可以提升MOM的的效率，我们将其固定为500，通过调节剩余的三个参数：学习率（Learning rate），批次数（Nbatchs）和负采样率(Negative samping rate)。所以MOMWH模型的参数设置如表\ref{tab:canhsu2}所示。
\begin{table}[!h]
\label{tab:canhsu2}
\caption{MOMWH模型中HORD超参调节的参数设置}
\centering
\begin{tabular}{cc}
\toprule  %添加表格头部粗线
\midrule  %添加表格中横线
参数     & 参数值 \\
\hline
$d$      & $\{{\rm{50}},{\rm{100}}\}$ \\
$d_M$    & $\{ {\rm{50}} \times {\rm{50}},{\rm{100}} \times {\rm{100}}\}$ \\
$nbatch$ & $\left[ {10,60} \right]$\\
$r$      & $\left[ {0.001,0.05} \right]$\\
$nrate$  & $\left[ {3,15} \right]$\\
$epoch$  & $500$\\
$n_0$    & $6$ \\
$N_{max}$& $25$\\
\hline
\bottomrule %添加表格底部粗线
\end{tabular}
\end{table}

\indent MOMWH中通过HORD算法进行超参优化后，MOM-S与MOM-R的最优超参配置对比如表所示：
\begin{table}[!h]
\label{tab:canhsu3}
\caption{HORD算法进行超参数优化前后的参数对比}
\centering
\begin{tabular}{ccc|cc}
\toprule  %添加表格头部粗线
\midrule  %添加表格中横线
\hline
参数名   & \multicolumn{2}{c|}{MOM-S}&\multicolumn{2}{c}{ MOM-R}\\        
         & 优化前   & 优化后   & 优化前  & 优化后 \\
\hline
$nbatch$ &   5      &   8      &  10     &   12 \\
$r$      &   0.0100 &   0.0131 &  0.0100 &   0.0107\\ 
$nrate$  &   3      &   6      &  5      &   8\\
$epoch$  &   1000   &   500    &  1000   &   500\\
\hline
\bottomrule %添加表格底部粗线
\end{tabular}
\end{table}


\begin{table}[!h]
	\centering
	\footnotesize
	\caption{MultiOMH与MultiOM以及各种基准匹配结果的对比图}
	\label{tab:result5}
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
		\multirow{Methods} &\multicolumn{5}{c|}{MA-NCI} & \multicolumn{5}{c|}{FMA-NCI-small} \\
		&Number & Correct& P &R & F1&Number & Correct& P &R & F1\\\hline
		StringEquiv & 935& 932& 0.997  & 0.615 &   0.761
		& 1501 & 1389 & 0.925  &0.517 &0.663   \\\hline
		StringEquiv-N &992 & 989&  0.997& 0.625 & 0.789
		& 1716 & 1598 & 0.931  &0.595 &  0.726 \\\hline
		StringEquiv-S &1100 & 1057& 0.961 & 0.697&  0.808
		& 2343 & 2082 & 0.889  &0.775 &  0.828 \\\hline
		StringEquiv-SR & 1162& 1094 & 0.941 & 0.722 &  0.817
		& 2343 & 2082 & 0.889  &0.775 &  0.828 \\\hline
		StringEquiv-NS & 1153& 1109& 0.962 & 0.732&   0.831
		& 2464 & 2200 & 0.893  &0.819 &  0.854  \\\hline
		StringEquiv-NSR &1211 & 1143  & 0.943  & 0.753 &  0.838
		& 2467 & 2203 & 0.893  &0.820 &  0.855  \\\hline
		MultiOM$^-$&1484 & 1296   & 0.873  & 0.855 &  0.864
		&  2471   &  2192  &  0.887  & 0.809   & 0.846    \\\hline
		MultiOM&1445 & 1287   & 0.891  & 0.849 &  0.869
		&  2470  & 2195  &    0.889 &   0.817 &   0.0.851  \\\hline
        MultiOMH$^-$&1462 & 1296   & 0.886  & 0.855 &  0.870
		&  2469   &  2192  &  0.888  & 0.809   & 0.847    \\\hline
        MultiOMH&1431 & 1290   & 0.901  & 0.851 &  0.0.876
		&  2466  & 2200  &    0.892 &   0.819 &   0.854  \\\hline
	\end{tabular}
\end{table}}

\begin{table}[!h]
	\centering
	\caption{MultiOMH与MultiOM分别将不同视角的模型组合后的效果对比图}
	\footnotesize
	\label{tab:result6}
	\begin{tabular}{|c|c|c|c|c|c|}
		\hline
		Methods &Number & Correct& P &R & F1\\\hline
		OM-$S^-$ (threshold$=0.95$)  & 1836& 1109      &    0.604   & 0.732  & 0.662    \\\hline
		OM-$S$ (threshold$=0.95$)    & 1189& 1097& 0.923  & 0.724 &   0.811  \\\hline
		OM-$R$ (Random initialization, threshold$=0.95$) &    709   &   680       &  0.959   &  0.449   &  0.612      \\\hline
		OM-$R$ (loss function \ref{Pretaining},  threshold$=0.95$)     & 833& 789   & 0.948    & 0.520 &   0.672  \\\hline
		OM-$RS^-$ (threshold$=0.95$)   & 1271& 1147& 0.902  & 0.757 &   0.823  \\\hline
		OM-$RS$ (threshold$=0.95$)   & 1237& 1138& 0.920  & 0.751 &   0.827  \\\hline
        MultiOM$^-$&1484 & 1296   & 0.873  & 0.855 &  0.864 \\\hline
        MultiOM&1445 & 1287   & 0.891  & 0.849 &  0.869  \\\hline
        OMH-$S^-$ (threshold$=0.95$)  & 1821& 1110      &    0.610   & 0.733  & 0.666    \\\hline
		OMH-$S$ (threshold$=0.95$)    & 1170& 1099& 0.939  & 0.725 &   0.818  \\\hline
		OMH-$R$ (Random initialization, threshold$=0.95$) &    705   &   680       &  0.965   &  0.449   &  0.613      \\\hline
		OMH-$R$ (loss function \ref{Pretaining},  threshold$=0.95$)     & 824& 794   & 0.964    & 0.524 &   0.679  \\\hline
		OMH-$RS^-$ (threshold$=0.95$)   & 1263& 1147& 0.908  & 0.757 &   0.826  \\\hline
		OMH-$RS$ (threshold$=0.95$)   & 1221& 1140& 0.934  & 0.752 &   0.833  \\\hline
		MultiOMH$^-$&1462 & 1296   & 0.886  & 0.855 &  0.870 \\\hline
		MultiOMH&1431 & 1290   & 0.901  & 0.851 &  0.876  \\\hline
	\end{tabular}
\end{table}

{\songti\xiaosihao 表\ref{tab:result5}列出了MultiOMH与MultiOM以及各种基准匹配结果的对比图。通过对比可以发现，采用HORD算法进行超参优化后MultiOMH 的效果较MultiOM无论是准确率，还是召回率都较之前有所提升，MultiOMH-的性能也优于MultiOM-，并且MultiOMH的结果可以与StringEquiv-NSR相当。MultiOMH是基于MultiOM采用HORD算法进行超参优化的模型，也进一步验证了表示学习模型超参数会影响模型所学特征，显然，经过优化后的参数，可以学到更加准确的特征；另外，无论是MultiOM与MultiOM-还是MultiOMH与MultiOMH-都说明同利用了改进后的结构负采样，效果依然会有所提高，也进一步验证了本体的结构信息有助于本体匹配任务效果的提升。
\indent 表\ref{tab:result6}展示了MultiOMH与MultiOM分别将不同视角的模型组合后的效果对比图。通过各个视角模型的匹配结果详细对比可以发现，采用HORD进行参数优化后，基本上对各个视角的匹配结果都有所提升。众多的结果说明，经过参数优化后，不仅可以使得各个模型错误的匹配得以减少同时可以寻找到一些正确匹配，以上结果进一步说明参数优化后可以帮助表示学习模型学得更加合适得特征，获得得概念向量表示更加有效。}


\subsection{MOMWH$^+$与MOMWH,MOM及其他方法结果对比}


\begin{table}[!h]
	\centering
	\caption{BertMultiOM+HORD与MultiOM，OAEI2018顶级系统以及基于表示学习的SCBOW+DAE模型的效果比较}
	\label{tab:result8}
	\footnotesize
	\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
		\hline
		\multirow{Methods} &\multicolumn{5}{c|}{MA-NCI} & \multicolumn{5}{c|}{FMA-NCI-small} \\
		  &Number & Correct& P &R & F1  &Number & Correct& P &R & F1\\\hline
		SCBOW + DAE 	& 1399 & 1356   & 0.969  & 0.906 &   0.938
		& 2282 & 2227  & 0.976 & 0.889    & 0.930 \\\hline
        \textbf{BertMultiOM+HORD}& \textbf{1436} & \textbf{1363}  & \textbf{0.949}  & \textbf{0.891} &  \textbf{0.919}
		&  \textbf{2787}  & \textbf{2647}  &    \textbf{0.950} &   \textbf{0.907} &   \textbf{0.928} \\\hline
		\textbf{BertMultiOM}& \textbf{1441} & \textbf{1363}  & \textbf{0.946}  & \textbf{0.891} &  \textbf{0.918}
		&  \textbf{2793}  & \textbf{2647}  &    \textbf{0.948} &   \textbf{0.907} &   \textbf{0.927} \\\hline
        LogMapBio   &1550 & 1376  &  0.888& 0.908 & 0.898
		& 2776  & 2632  & 0.948  & 0.902   & 0.921 \\\hline
		POMAP++     &1446 &  1329 &  0.919& 0.877 & 0.897
		&2414  &2363  & 0.979  & 0.814    & 0.889 \\\hline
		XMap             &1413 &   1312& 0.929 & 0.865&  0.896
		&2315  & 2262 & 0.977  & 0.783   & 0.869 \\\hline
		LogMap        & 1387&   1273  & 0.918 & 0.846&   0.880
        & 2747 & 2593  & 0.944 &  0.897  & 0.920\\\hline
		SANOM         &1450 & 1287   & 0.888 & 0.844&  0.865
		& --  & --  & -- & --   & -- \\\hline
		FCAMapX      & 1274&  1199   & 0.941 & 0.791 &  0.859
		& 2828 &  2681  & 0.948 & 0.911   & 0.929 \\\hline
		\textbf{MultiOM}& \textbf{1445} & \textbf{1287}  & \textbf{0.891}  & \textbf{0.849} &  \textbf{0.869}
		&  \textbf{2330}  & \textbf{2195}  &    \textbf{0.942} &   \textbf{0.817} &   \textbf{0.875} \\\hline
	\end{tabular}
\end{table}
\indent 但以上结果较一些比较好的本体匹配系统得效果依然存在较大差距，通过两个实验结果综合对比可以发现，利用改进得TF-IDF算法的效果准确率有$0.991$,其召回率不高，主要是因为我们利用的是预训练词向量，只包含其字面语义信息，并未对这些预训练词向量基于当前语境进行改进，所以而未包含该本体匹配任务下的语义信息。考虑到以上原因，本文进一步在MultiOM框架下，采用BERT模型进行改进，我们将改进后的模型命名为BertMultiOM，BertMultiOM与MultiOM，OAEI2018顶级系统以及基于表示学习的SCBOW+DAE模型的效果比较见表\ref{tab:result7}。


\indent 根据表\ref{tab:result7}中的结果对比可以发现，BertMultiOM的匹配效果仅仅次于SCBOW+DAE模型的效果，并且较SCBOW+DAE模型，可以找到更多正确匹配；与第二名LogMapBio系统相比，BertMultiOM的正确率与F值要优于LogMapBio，因此，通过该方式对MultiOM的改进是有效的。之前的实验已经验证了HORD算法通过参数优化可以提升MultiOM的匹配效果，而使用了BERT模型后，效果虽然可以提升，但模型的效果依然受到参数的影响，故采用HORD算法进一步对BertMultiOM模型的超参进行调节。因考虑到BertMultiOM模型训练的时间成本，故本节只对BertMultiOM模型的学习率进行优化，改进BertMultiOM学习率后的效果对比如图\ref{tab:result8}所示，BertMultiOM+HORD的匹配效果较参数优化前的BertMultiOM要略好，在数据集FMA-NCI-small中，也近似得到了第一名SCBOW+DAE模型的效果。以上实验进一步说明对表示学习模型的超参数进行优化，使得表示学习模型学到更优的特征，概念的向量表示也更加合适，同时也有助于提升本体匹配模型MultiOM的性能。

{\songti\xiaosihao 
\indent 为突出我们提出的负采样的效果，在模块标签或合并标签上添加符号“-”表示该模块未配备针对结构关系量身定制的负采样。




}



\indent -OM-L：MultiOM+HORD未对基于上下文视角的预训练模型进行超参优化，所以依然沿用OM-L表示改进后的TF-IDF算法的效果；

\indent -OMH-R：MultiOM中采用OM-R表示基于外部资源视角的预训练模型的匹配结果，MultiOM+HORD采用HORD算法对其参数进行优化，故这里简记为OMH-R；

\indent -OMH-S：MultiOM中采用OM-S表示基于结构视角的预训练模型的匹配结果，MultiOM+HORD采用HORD算法对其参数进行优化，故这里简记为OMH-S；

\indent -OMH-RS：是OMH-R与OMH-S采用多视角匹配算法进行组合后的结果；

\indent -OMH-SR$^-$：未采用本文改进的结构负采样，将OMH-R与OMH-S 采用多视角匹配算法进行组合后的结果；

\indent -MultiOMH：以同样的方式，这里采用MultiOMH表示HORD算法改进MultiOM后的整体结果；MultiOMH$^-$表示MultiOM未采用本文改进的结构负采样的结果。




}\\
\ifx\allfiles\undefined
\end{document}
\fi
